{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qXN_w6U50aUs",
        "J08qfUWm0NUz",
        "hM6zsbYn0Qmg",
        "6bX9l1Ar0juC",
        "R0po15FNy1Mc"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **UNet Grad-CAM for Infant Ultrasound Body Composition**\n",
        "\n",
        "This notebook trains a UNet with the same number of epochs and settings as the main model, but is tailored for generating Grad-CAM explanations of FM/FFM regression. It uses preprocessed clinical ultrasound images (no PHI in-notebook), enforces 4D (N,C,H,W) inputs, and preserves decoder feature maps for Grad-CAM visualization. Training matches the standard setup (not a toy run), ensuring Grad-CAM outputs are representative of the full model performance.\n",
        "\n",
        "The notebook contains Grad-CAM methods utilized in the paper titled: Enhancing Newborn Health Assessment: Ultrasound-based Body Composition Prediction Using Deep Learning Techniques in the journal Ultrasound in Medicine & Biology."
      ],
      "metadata": {
        "id": "qXN_w6U50aUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The following cell defines all nessecary packages used to run the code.\n",
        "\n",
        "Additionally, Google Drive is mounted to the notebook. The Drive contains ultrasound images of body regions corresponding to infants.  "
      ],
      "metadata": {
        "id": "J08qfUWm0NUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Importing all nessecary packages and tools\n",
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import torch\n",
        "import random\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from datetime import date\n",
        "from skimage import io, color\n",
        "from google.colab import drive\n",
        "from torchvision import models\n",
        "import skimage.morphology as mo\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image, ImageFilter\n",
        "from scipy.ndimage import median_filter\n",
        "from torch.utils.data import random_split\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data  import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "# from utils import *\n",
        "# import timm\n",
        "# from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "# import types\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4LTjjK7zYq7",
        "outputId": "3488efc0-071a-4553-96df-00eeb2dac702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Necessary Pre-Processing\n",
        "\n",
        "The following four cells mount the proper folders and sets up the data and the patients with matching body composition metric. To do this, there must be another file, specifically a csv file which contains a nominal list of all patients, body composition metrics which can then be mapped to folders of corresponding patient_id.\n",
        "\n",
        "The ultrasound data is not publically available."
      ],
      "metadata": {
        "id": "hM6zsbYn0Qmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Step 1: Count total number of images across all patients in the dataset folder**\n"
      ],
      "metadata": {
        "id": "9PBYOkvl0UKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Root directory containing all patient subfolders with cropped ultrasound images\n",
        "root_dir = '/content/gdrive/MyDrive/Ultrasound Files- Minnesota + Boston Collaboration/cropped_images_bluethingremoved'\n",
        "# List of patient subdirectories (each folder corresponds to one patient)\n",
        "patients = os.listdir(root_dir)\n",
        "# Count total number of image files across all patients\n",
        "total = 0\n",
        "for patient in patients:\n",
        "  total += len(os.listdir(os.path.join(root_dir, patient)))\n",
        "print(total) # should be 721"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOewKO8f0RIH",
        "outputId": "706ae898-85e9-4d9c-c95e-7ee8051d9af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step 2: Load patient metadata from CSV and compare with folder structure**\n",
        "\n",
        "Here, we read csv data into a dataframe and collect all patient 'Study ID'. We print out number of patients in csv file and in folder. The number of patients in folder determine our dataset size for training and testing."
      ],
      "metadata": {
        "id": "7-fZKN-I0U56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read metadata file containing patient IDs and body composition measurements\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/Ultrasound Files- Minnesota + Boston Collaboration/Data_11.5.23_modified.csv')\n",
        "# Extract Study IDs from the CSV\n",
        "patients = df['Study_ID']\n",
        "# Get list of patient folders in the cropped_images directory\n",
        "folders = os.listdir('/content/gdrive/MyDrive/Ultrasound Files- Minnesota + Boston Collaboration/cropped_images') #cropped_images_bluethingremoved\n",
        "#Confirm matching number of patients\n",
        "print('Number of patients in csv file: ', len(patients))\n",
        "print('Number of patients in folder: ', len(folders))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BuYqow10W23",
        "outputId": "cdf72c95-e93b-4389-912c-b93f8638c327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of patients in csv file:  65\n",
            "Number of patients in folder:  65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Build dictionaries for labels (FM, FFM, Weight, Length)**\n",
        "\n",
        "Here, create two dictionary, using 'Study ID' as key, and ['FM', 'FFM', 'Weight_visit', 'Length_visit'] as value. Create the test dictionary from the training dictionary."
      ],
      "metadata": {
        "id": "V6SfIrHT0YZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary mapping Study_ID -> [FM, FFM, Weight_visit, Length_visit]\n",
        "labels_dict = df.set_index('Study_ID')[['FM', 'FFM', 'Weight_visit', 'Length_visit']].apply(list, axis=1).to_dict()\n",
        "# Define which patient IDs will be reserved for testing\n",
        "test_data = [17042418, 9021218, 35080318, 58041919, 65050619, 57032919, 50102518]#, , , 57032919, 67062119, 60042419, 44091318]\n",
        "# Create test dictionary from training dictionary\n",
        "test_labels_dict = {k: labels_dict[k] for k in test_data if k in labels_dict}\n",
        "# Remove test patients from training dictionary\n",
        "for i in test_data:\n",
        "  del labels_dict[i]\n",
        "PATIENTS = [str(num) for num in test_data]\n",
        "\n",
        "print(len(labels_dict))\n",
        "print(len(test_labels_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zeh1vvg40aqb",
        "outputId": "7d932095-1a87-4b5a-f29b-a2bffbafe722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58\n",
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: The cell below checks that there's no missing images in the folder left.**"
      ],
      "metadata": {
        "id": "-ixw-kca0dvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming each patient in the dictionaries has a list of image paths as one of the values.\n",
        "# If the dictionary structure is different, this code will need adjustment.\n",
        "\n",
        "total_train_images = 0\n",
        "for patient_id, data in labels_dict.items():\n",
        "\n",
        "    patient_path = os.path.join(root_dir, str(patient_id))\n",
        "    if os.path.exists(patient_path):\n",
        "        total_train_images += len(os.listdir(patient_path))\n",
        "    else:\n",
        "        print(f\"Warning: Directory not found for patient {patient_id}\")\n",
        "\n",
        "\n",
        "total_test_images = 0\n",
        "for patient_id, data in test_labels_dict.items():\n",
        "    patient_path = os.path.join(root_dir, str(patient_id))\n",
        "    if os.path.exists(patient_path):\n",
        "        total_test_images += len(os.listdir(patient_path))\n",
        "    else:\n",
        "        print(f\"Warning: Directory not found for test patient {patient_id}\")\n",
        "\n",
        "print(\"Total number of images for training dataset:\", total_train_images)\n",
        "print(\"Total number of images for test dataset:\", total_test_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS99nRLO0ezj",
        "outputId": "c2d16ace-09cc-4fee-f58e-6f0d0bbc0268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Directory not found for patient 1111517\n",
            "Warning: Directory not found for patient 2111617\n",
            "Warning: Directory not found for patient 3120117\n",
            "Warning: Directory not found for patient 4120117\n",
            "Warning: Directory not found for patient 7121817\n",
            "Warning: Directory not found for patient 8021218\n",
            "Total number of images for training dataset: 581\n",
            "Total number of images for test dataset: 75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##In the next following section, we define the Grad-CAM data-loading, training and statistics functions.\n",
        "\n",
        "These functions are used primarily to split and process the ultrasound images into training and testing groups. The PatientDataset function and TestPatientDataset function are both used to process ultrasound images so that they are able to be thread into the deep learning models. Some modifications of the models are done in subsequent blocks due to shape differences in the first layer of the deep learning models.\n",
        "\n",
        "The training function is also defined in this section. In this function, the loss function is defined, along with the optimizer. The data is parsed and assigned to be trained by a specific model which is indicated in the training function. The training function utilizes a data-loader, which then keeps track of the training and validation losses as the training begins.\n",
        "\n",
        "Loss functions, and metric functions (including MAPE, MSE, RSME and MAE) are defined and instantiated in the sequence below too. These are used to quantify the performance of our models."
      ],
      "metadata": {
        "id": "6bX9l1Ar0juC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patient Dataset (Grad-CAM Compatible)"
      ],
      "metadata": {
        "id": "5owZjOQyzNDR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p_vUe_Fx1W3"
      },
      "outputs": [],
      "source": [
        "REGION_IDX = {\"B\": 0, \"A\": 1, \"Q\": 2}\n",
        "\n",
        "class PatientDataset(Dataset):\n",
        "    \"\"\"\n",
        "      PyTorch Dataset for loading cropped ultrasound images and associated patient labels\n",
        "      for a single specified region (Biceps, Abdomen, or Quadriceps).\n",
        "\n",
        "      This dataset handles:\n",
        "      - Selecting valid patient folders from the cropped dataset directory\n",
        "      - Excluding hold-out patients (if `test_data` global is defined)\n",
        "      - Loading a single region image per patient (B, A, or Q)\n",
        "      - Applying optional preprocessing (cropping, speckle reduction, thresholding, denoising)\n",
        "      - Returning the requested target output (FM or FFM) along with weight and length\n",
        "\n",
        "      Args:\n",
        "          transform (callable, optional):\n",
        "              Image transform/augmentation function applied after loading.\n",
        "          augmented_dataset (bool):\n",
        "              If True, apply augmentation to the *entire dataset* (not used in this override).\n",
        "          augment (int):\n",
        "              Level of augmentation (default 0 = none).\n",
        "          threshold (bool):\n",
        "              If True, apply pixel thresholding for noise reduction.\n",
        "          speckle (bool):\n",
        "              If True, apply median filtering for speckle noise reduction.\n",
        "          despeckle (bool):\n",
        "              If True, apply fast Non-Local Means denoising.\n",
        "          region (str):\n",
        "              Which region to load:\n",
        "                  \"B\" = Biceps,\n",
        "                  \"A\" = Abdomen,\n",
        "                  \"Q\" = Quadriceps.\n",
        "          number_of_image (int):\n",
        "              Number of images to load per region (default = 1).\n",
        "          crop (list of float):\n",
        "              Fraction of each image to keep, indexed by region:\n",
        "                  [Abdomen, Biceps, Quadriceps].\n",
        "              Example: [0.8, 1, 0.5] keeps 80% of Abdomen, 100% of Biceps, 50% of Quadriceps.\n",
        "          output (str):\n",
        "              Target label to predict:\n",
        "                  \"FM\" = Fat Mass,\n",
        "                  \"FFM\" = Fat-Free Mass.\n",
        "\n",
        "      Returns:\n",
        "          image (torch.Tensor): Preprocessed ultrasound image tensor for the selected region.\n",
        "          label (torch.Tensor): Target label (FM or FFM) as shape (1,).\n",
        "          weight (torch.Tensor): Patient's body weight from metadata as shape (1,).\n",
        "          length (torch.Tensor): Patient's body length from metadata as shape (1,).\n",
        "    \"\"\"\n",
        "    def __init__(self, transform=None, augmented_dataset=False, augment=0, threshold=False, speckle=False,\n",
        "                 despeckle=False, region=\"B\", number_of_image=1, crop=[1, 1, 1], output=\"FM\"):\n",
        "        self.root_dir = '/content/gdrive/MyDrive/Ultrasound Files- Minnesota + Boston Collaboration/cropped_images'\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.threshold = threshold\n",
        "        self.speckle = speckle\n",
        "        self.augmented_dataset = augmented_dataset\n",
        "        self.despeckle = despeckle\n",
        "        self.region = region\n",
        "        self.number_of_image = number_of_image\n",
        "        self.crop = crop\n",
        "        self.output = output\n",
        "\n",
        "        # Build a clean list of valid patient IDs up front\n",
        "        all_ids = [p for p in os.listdir(self.root_dir) if p.isdigit()]\n",
        "        # Optionally exclude hold-out IDs\n",
        "        holdout = set(map(str, test_data)) if 'test_data' in globals() else set()\n",
        "        candidates = [pid for pid in all_ids if pid not in holdout]\n",
        "\n",
        "        self.patients = []\n",
        "        for pid in candidates:\n",
        "            path = os.path.join(self.root_dir, pid)\n",
        "            if any(f\"_{self.region}\" in fn for fn in os.listdir(path)):\n",
        "                self.patients.append(pid)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patients)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        patient_id = self.patients[idx]\n",
        "        patient_path = os.path.join(self.root_dir, patient_id)\n",
        "\n",
        "        # image tensor (3, H, W)\n",
        "        image = self.load_patient_data(patient_path)\n",
        "\n",
        "        # label/aux as float32 tensors (shape (1,))\n",
        "        label  = torch.tensor(self.get_label(int(patient_id)), dtype=torch.float32).view(1)\n",
        "        weight = torch.tensor(labels_dict[int(patient_id)][2], dtype=torch.float32).view(1)\n",
        "        length = torch.tensor(labels_dict[int(patient_id)][3], dtype=torch.float32).view(1)\n",
        "\n",
        "        return image, label, weight, length\n",
        "\n",
        "    def get_label(self, patient_id):\n",
        "        if self.output in (\"FM\", \"PerFM\"):\n",
        "            return labels_dict[patient_id][0]\n",
        "        if self.output in (\"FFM\", \"PerFFM\"):\n",
        "            return labels_dict[patient_id][1]\n",
        "        raise ValueError(f\"Invalid output type: {self.output}\")\n",
        "\n",
        "    def preprocess(self, img_path, crop_frac):\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        w, h = image.size\n",
        "        image = image.crop((0, 0, w, round(h * float(crop_frac))))\n",
        "        if self.speckle:\n",
        "            image = image.filter(ImageFilter.MedianFilter(size=5))\n",
        "        if self.threshold:\n",
        "            arr = np.array(image)\n",
        "            arr = np.where(arr < 100, 0, arr)\n",
        "            image = Image.fromarray(arr)\n",
        "        if self.despeckle:\n",
        "            arr = np.array(image)\n",
        "            arr = cv2.fastNlMeansDenoisingColored(arr, None, h=10, templateWindowSize=7, searchWindowSize=21)\n",
        "            image = Image.fromarray(arr)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)  # -> Tensor (3,H,W)\n",
        "        return image\n",
        "\n",
        "    def load_patient_data(self, patient_path):\n",
        "        r = self.region\n",
        "        crop_idx = REGION_IDX.get(r, 0)\n",
        "        for img_file in os.listdir(patient_path):\n",
        "            if f\"_{r}\" in img_file:\n",
        "                img_path = os.path.join(patient_path, img_file)\n",
        "                return self.preprocess(img_path, self.crop[crop_idx])\n",
        "        raise ValueError(f\"No image found for region {r} in {patient_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Patient Dataset (Grad-CAM Compatible)"
      ],
      "metadata": {
        "id": "CWEiP9oazIl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestPatientDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for loading cropped ultrasound images and labels for a fixed\n",
        "    set of test patients.\n",
        "\n",
        "    This dataset handles:\n",
        "    - Selecting patient folders from a provided test patient list (`PATIENTS`)\n",
        "    - Loading multiple regions per patient (Abdomen, Biceps, Quadriceps) depending on `region_combination`\n",
        "    - Applying optional preprocessing (cropping, speckle reduction, thresholding, denoising)\n",
        "    - Applying optional data augmentation (horizontal/vertical flips, rotations) based on `augment` level\n",
        "    - Returning the requested target output (FM or FFM, absolute or percentage) along with weight and length\n",
        "\n",
        "    Args:\n",
        "        transform (callable, optional):\n",
        "            Image transform/augmentation function applied after loading.\n",
        "        augmented_dataset (bool):\n",
        "            If True, apply augmentation to the *entire dataset* instead of per-image.\n",
        "        augment (int):\n",
        "            Level of augmentation:\n",
        "                0 = none,\n",
        "                5 = flip + rotation augmentations,\n",
        "                8 = stronger flips/rotations,\n",
        "                15 = extensive rotation set.\n",
        "        threshold (bool):\n",
        "            If True, apply pixel thresholding for noise reduction.\n",
        "        speckle (bool):\n",
        "            If True, apply median filtering for speckle noise reduction.\n",
        "        despeckle (bool):\n",
        "            If True, apply fast Non-Local Means denoising.\n",
        "        region_combination (str):\n",
        "            Which regions to include:\n",
        "                \"B\" = Biceps,\n",
        "                \"A\" = Abdomen,\n",
        "                \"Q\" = Quadriceps,\n",
        "                \"BA\", \"BQ\", \"AQ\", \"BAQ\" = multiple regions.\n",
        "        number_of_image (int):\n",
        "            Number of images to load per region (1, 2, or 3).\n",
        "        crop (list of float):\n",
        "            Fraction of each image to keep for [Abdomen, Biceps, Quadriceps].\n",
        "            Example: [0.8, 1, 0.5] keeps 80% of Abdomen, full Biceps, 50% Quadriceps.\n",
        "        output (str):\n",
        "            Target label to predict:\n",
        "                \"FM\"    = Fat Mass,\n",
        "                \"FFM\"   = Fat-Free Mass,\n",
        "                \"PerFM\" = Percent Fat Mass,\n",
        "                \"PerFFM\"= Percent Fat-Free Mass.\n",
        "\n",
        "    Returns:\n",
        "        images (list[torch.Tensor]): List of preprocessed ultrasound images for the patient.\n",
        "        label (float): Target label (FM, FFM, or percentage).\n",
        "        weight (float): Patient's body weight from metadata.\n",
        "        length (float): Patient's body length from metadata.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transform=None, augmented_dataset=False, augment=0, threshold=False, speckle=False,\n",
        "                 despeckle=False, region_combination='A', number_of_image=1, crop=[1, 1, 1], output=\"FM\"):\n",
        "        self.root_dir = '/content/gdrive/MyDrive/Ultrasound Files- Minnesota + Boston Collaboration/cropped_images'\n",
        "        self.patients = PATIENTS\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.threshold = threshold\n",
        "        self.speckle = speckle\n",
        "        self.augmented_dataset = augmented_dataset\n",
        "        self.despeckle = despeckle\n",
        "        self.region_combination = region_combination\n",
        "        self.number_of_image = number_of_image\n",
        "        # Crop in the order of Abdomen, Biceps, and Quadriceps\n",
        "        self.crop = crop\n",
        "        self.output = output\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patients)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        patient_id = self.patients[idx]\n",
        "        patient_path = os.path.join(self.root_dir, patient_id)\n",
        "        images = self.load_patient_data(patient_path)\n",
        "        if self.output == \"FM\":\n",
        "            return images, test_labels_dict[int(patient_id)][0], test_labels_dict[int(patient_id)][2], test_labels_dict[int(patient_id)][3]\n",
        "        elif self.output == \"FFM\":\n",
        "            return images, test_labels_dict[int(patient_id)][1], test_labels_dict[int(patient_id)][2], test_labels_dict[int(patient_id)][3]\n",
        "        elif self.output == \"PerFM\":\n",
        "            return images, test_labels_dict[int(patient_id)][0], test_labels_dict[int(patient_id)][2], test_labels_dict[int(patient_id)][3]\n",
        "        elif self.output == \"PerFFM\":\n",
        "            return images, test_labels_dict[int(patient_id)][1], test_labels_dict[int(patient_id)][2], test_labels_dict[int(patient_id)][3]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid output type: {self.output}\")\n",
        "\n",
        "    def preprocess(self, img_path, MF_size, images, threshold, h, templateWindowSize, searchWindowSize, crop):\n",
        "        image = Image.open(img_path)\n",
        "        width, height = image.size\n",
        "        crop_rectangle = (0, 0, width, round(height * crop))\n",
        "        image = image.crop(crop_rectangle)\n",
        "        if self.speckle:\n",
        "            image = image.filter(ImageFilter.MedianFilter(size=MF_size))\n",
        "        if self.threshold:\n",
        "            image = torch.where(image < threshold / 255, torch.tensor(0.0), image)\n",
        "        if self.despeckle:\n",
        "            image_np = np.array(image)\n",
        "            despeckled_image = cv2.fastNlMeansDenoisingColored(image_np, None, h=h, templateWindowSize=templateWindowSize, searchWindowSize=searchWindowSize)\n",
        "            image = Image.fromarray(despeckled_image)\n",
        "        if not self.augmented_dataset:\n",
        "            image = self.transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "        # Augmentation\n",
        "        if self.augment >= 5:\n",
        "            imx = image\n",
        "            imx = TF.hflip(imx)\n",
        "            images.append(imx)\n",
        "            imx = TF.vflip(imx)\n",
        "            images.append(imx)\n",
        "            angle = random.choice([-30, -90, -60, -45, -15, 0, 15, 30, 45, 60, 90])\n",
        "            imx = TF.rotate(imx, angle)\n",
        "            images.append(imx)\n",
        "            angle = random.choice([-30, -90, -60, -45, -15, 0, 15, 30, 45, 60, 90])\n",
        "            imx = TF.rotate(imx, angle)\n",
        "            images.append(imx)\n",
        "        if self.augment >= 8:\n",
        "            imx = TF.vflip(imx)\n",
        "            images.append(imx)\n",
        "            angle = random.choice([15])\n",
        "            imx = TF.rotate(imx, angle)\n",
        "            images.append(imx)\n",
        "            angle = random.choice([45])\n",
        "            imx = TF.rotate(imx, angle)\n",
        "            images.append(imx)\n",
        "        if self.augment >= 15:\n",
        "            imx = image\n",
        "            angle = random.choice([-30, -90, -60, -45, -15, 0, 15, 30, 45, 60, 90])\n",
        "            imx = TF.rotate(imx, angle)\n",
        "            images.append(imx)\n",
        "            imx = image\n",
        "            angle = random.choice([-90])\n",
        "            imx = TF.rotate(imx, angle)\n",
        "            images.append(imx)\n",
        "            imx = image\n",
        "            angle = random.choice([-60])\n",
        "            imx = TF.rotate(imx, angle)\n",
        "            images.append(imx)\n",
        "            angle = random.choice([-45])\n",
        "            imx = TF.rotate(imx, angle)\n",
        "            images.append(imx)\n",
        "            angle = random.choice([-15])\n",
        "            imx = TF.rotate(imx, angle)\n",
        "            images.append(imx)\n",
        "            angle = random.choice([30])\n",
        "            imx = TF.rotate(imx, angle)\n",
        "            images.append(imx)\n",
        "            angle = random.choice([60])\n",
        "            imx = TF.rotate(imx, angle)\n",
        "            images.append(imx)\n",
        "        return images\n",
        "\n",
        "    def load_patient_data(self, patient_path):\n",
        "        images = []\n",
        "        threshold = 100\n",
        "        AB = 0\n",
        "        BICEP = 0\n",
        "        QUAD = 0\n",
        "        MF_size = 5\n",
        "        h = 10\n",
        "        templateWindowSize = 7\n",
        "        searchWindowSize = 21\n",
        "        for img_file in os.listdir(patient_path):\n",
        "            # Process Abdomen images\n",
        "            if 'A' in self.region_combination and '_A' in img_file and AB < self.number_of_image:\n",
        "                img_path = os.path.join(patient_path, img_file)\n",
        "                images = self.preprocess(img_path, MF_size, images, threshold, h, templateWindowSize, searchWindowSize, self.crop[0])\n",
        "                AB += 1\n",
        "            # Process Bicep images\n",
        "            elif 'B' in self.region_combination and '_B' in img_file and BICEP < self.number_of_image:\n",
        "                img_path = os.path.join(patient_path, img_file)\n",
        "                images = self.preprocess(img_path, MF_size, images, threshold, h, templateWindowSize, searchWindowSize, self.crop[1])\n",
        "                BICEP += 1\n",
        "            # Process Quad images\n",
        "            elif 'Q' in self.region_combination and '_Q' in img_file and QUAD < self.number_of_image:\n",
        "                img_path = os.path.join(patient_path, img_file)\n",
        "                images = self.preprocess(img_path, MF_size, images, threshold, h, templateWindowSize, searchWindowSize, self.crop[2])\n",
        "                QUAD += 1\n",
        "        return images"
      ],
      "metadata": {
        "id": "xt4YQlAlyF7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def im_converterX(tensor):\n",
        "  \"\"\"\n",
        "    Convert a PyTorch tensor into a NumPy image (H x W x C) for visualization.\n",
        "\n",
        "    Steps:\n",
        "        1. Move tensor to CPU (if on GPU).\n",
        "        2. Detach from computation graph (so gradients are not tracked).\n",
        "        3. Convert to NumPy array.\n",
        "        4. Rearrange dimensions from (C, H, W) -> (H, W, C).\n",
        "        5. Scale pixel values into [0, 1] range and clip.\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor):\n",
        "            Image tensor in CHW format, typically with values normalized\n",
        "            to [0, 1] or [-1, 1].\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray:\n",
        "            Image in HWC format with pixel values clipped between 0 and 1.\n",
        "    \"\"\"\n",
        "  image = tensor.cpu().clone().detach().numpy()\n",
        "  image = image.transpose(1,2,0)\n",
        "  image = image * np.array((1, 1, 1))\n",
        "  image = image.clip(0, 1)\n",
        "  return image"
      ],
      "metadata": {
        "id": "SSD0j0Aw1NBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MAPELoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Mean Absolute Percentage Error (MAPE) Loss for regression tasks.\n",
        "\n",
        "    This loss is useful when the scale of the target values varies a lot,\n",
        "    because it expresses error as a percentage of the true value.\n",
        "\n",
        "    Formula:\n",
        "        MAPE = mean( |(target - pred) / (target + epsilon)| ) * 100\n",
        "\n",
        "    Args:\n",
        "        epsilon (float, optional): Small constant to avoid division by zero\n",
        "                                   when target values are close to 0. Default = 1e-7.\n",
        "\n",
        "    Forward Args:\n",
        "        pred (torch.Tensor): Predicted values from the model.\n",
        "        target (torch.Tensor): Ground truth values.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Scalar tensor containing the MAPE loss (percentage).\n",
        "    \"\"\"\n",
        "    def __init__(self, epsilon=1e-7):\n",
        "        super(MAPELoss, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        return torch.mean(torch.abs((target - pred) / (target + self.epsilon))) * 100"
      ],
      "metadata": {
        "id": "cOYhJ3Zl1OgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom regression loss that combines Mean Squared Error (MSE) and\n",
        "    Mean Absolute Error (L1/MAE) using weighted averaging.\n",
        "\n",
        "    Motivation:\n",
        "        - MSE penalizes large errors more strongly (sensitive to outliers).\n",
        "        - L1 (MAE) is more robust to outliers but less sensitive to small differences.\n",
        "        - Combining both gives a balanced trade-off.\n",
        "\n",
        "    Formula:\n",
        "        total_loss = weight_mse * MSE(input, target)\n",
        "                   + weight_l1  * L1(input, target)\n",
        "\n",
        "    Args:\n",
        "        weight_mse (float): Weight for the MSE component. Default = 0.6\n",
        "        weight_l1 (float): Weight for the L1 component.  Default = 0.4\n",
        "\n",
        "    Forward Args:\n",
        "        input (torch.Tensor): Predicted values from the model.\n",
        "        target (torch.Tensor): Ground truth values.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Scalar tensor containing the weighted loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight_mse=0.6, weight_l1=0.4):\n",
        "        super(CustomLoss, self).__init__()\n",
        "        self.weight_mse = weight_mse\n",
        "        self.weight_l1 = weight_l1\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        mse_loss = self.mse_loss(input, target)\n",
        "        l1_loss = self.l1_loss(input, target)\n",
        "        total_loss = self.weight_mse * mse_loss + self.weight_l1 * l1_loss\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "df7E5yT01P0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(output, target):\n",
        "    mse_loss = nn.MSELoss()(output, target)\n",
        "    penalty = torch.mean(F.relu(-output))  # Penalize negative predictions\n",
        "    return mse_loss + penalty"
      ],
      "metadata": {
        "id": "zw9rGiTq1TTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Grad-CAM training function trains and validates the model for the same number of epochs as the main pipeline, while enforcing 4D (N,C,H,W) inputs and preserving gradients needed for CAM.\n",
        "It supports MSE, MAE, and MAPE losses, works with both UNet and EffNet variants, and can optionally incorporate patient metadata (weight, length) during the forward pass.\n",
        "Optimization uses Adam with optional ExponentialLR scheduling; the loop runs on GPU, logs batch/epoch losses, and returns train/validation loss histories for reproducible CAM generation."
      ],
      "metadata": {
        "id": "RQAtj0_EzCNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training(epochs, criterion, lr, model, MODEL, adaptive=True, weight_length=False):\n",
        "    \"\"\"\n",
        "      Train and validate a Grad-CAM compatible model using selected loss function,\n",
        "      optimizer, and optional adaptive learning rate scheduling. Ensures 4D inputs\n",
        "      and preserves gradients for CAM visualization.\n",
        "\n",
        "      Args:\n",
        "          epochs (int): Number of training epochs.\n",
        "          criterion (str): Loss function to use. Options:\n",
        "              - \"MSE\"  -> Mean Squared Error\n",
        "              - \"MAE\"  -> Mean Absolute Error\n",
        "              - \"MAPE\" -> Mean Absolute Percentage Error\n",
        "          lr (float): Learning rate for optimizer.\n",
        "          model (nn.Module): PyTorch model to be trained.\n",
        "          MODEL (str): Model type identifier (\"EffNet_LP\", \"EffNet_FT\", \"UNet\").\n",
        "          adaptive (bool, optional): If True, applies exponential LR decay (gamma=0.95).\n",
        "                                    Default = True.\n",
        "          weight_length (bool, optional): If True, includes patient metadata (weight & length)\n",
        "                                          in the forward pass. Default = False.\n",
        "\n",
        "      Returns:\n",
        "          tuple: (train_running_loss_history, validation_running_loss_history)\n",
        "              - train_running_loss_history (list[float]): Avg. training loss per epoch.\n",
        "              - validation_running_loss_history (list[float]): Avg. validation loss per epoch.\n",
        "    \"\"\"\n",
        "    if criterion == \"MSE\":\n",
        "        criterion = nn.MSELoss()\n",
        "    elif criterion == \"MAE\":\n",
        "        criterion = nn.L1Loss()\n",
        "    elif criterion == \"MAPE\":\n",
        "        criterion = MAPELoss()\n",
        "\n",
        "    # Move the model to GPU\n",
        "    model = model.to('cuda')\n",
        "\n",
        "    ### Optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    if adaptive:\n",
        "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "    train_running_loss_history = []\n",
        "    validation_running_loss_history = []\n",
        "\n",
        "    for e in range(epochs):\n",
        "        train_running_loss = 0.0\n",
        "        validation_running_loss = 0.0\n",
        "        model.train()\n",
        "        for ith_batch, batch in enumerate(train_loader):\n",
        "            # Ensure X_train is properly shaped\n",
        "            if isinstance(batch[0], torch.Tensor):\n",
        "                X_train = batch[0].to('cuda').float()\n",
        "            else:\n",
        "                X_train = torch.stack([img for img in batch[0]]).to('cuda').float()\n",
        "\n",
        "            # Remove extra dimensions if present\n",
        "            if X_train.dim() == 5:  # If it's a 5D tensor, squeeze out unnecessary dimension\n",
        "                X_train = X_train.squeeze(1)\n",
        "\n",
        "            if weight_length:\n",
        "                if MODEL == \"EffNet_LP\" or MODEL == \"EffNet_FT\":\n",
        "                    additional_data_train = torch.tensor([[batch[2].item(), batch[3].item()]]).to('cuda').float()\n",
        "                elif MODEL == \"UNet\":\n",
        "                    weight = batch[2].to('cuda').float()[0]\n",
        "                    length = batch[3].to('cuda').float()[0]\n",
        "                y_train = batch[1].to('cuda').float()[0]\n",
        "                y_pred = model(X_train, weight, length)[0]\n",
        "            else:\n",
        "                y_train = batch[1].to('cuda').float()[0]\n",
        "                y_pred = model(X_train)[0]\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(y_pred, y_train)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if ith_batch % 5 == 0:\n",
        "                print('Epoch: ', e + 1, 'Batch: ', ith_batch, 'Current Loss: ', loss.item())\n",
        "\n",
        "            train_running_loss += loss.item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for ith_batch, batch in enumerate(test_loader):\n",
        "                # Ensure X_val is properly shaped\n",
        "                if isinstance(batch[0], torch.Tensor):\n",
        "                    X_val = batch[0].to('cuda').float()\n",
        "                else:\n",
        "                    X_val = torch.stack([img for img in batch[0]]).to('cuda').float()\n",
        "\n",
        "                # Remove extra dimensions if present\n",
        "                if X_val.dim() == 5:  # If it's a 5D tensor, squeeze out unnecessary dimension\n",
        "                    X_val = X_val.squeeze(1)\n",
        "\n",
        "                if weight_length:\n",
        "                    if MODEL == \"EffNet_LP\" or MODEL == \"EffNet_FT\":\n",
        "                        additional_data_val = torch.tensor([[batch[2].item(), batch[3].item()]]).to('cuda').float()\n",
        "                    elif MODEL == \"UNet\":\n",
        "                        weight = batch[2].to('cuda').float()[0]\n",
        "                        length = batch[3].to('cuda').float()[0]\n",
        "                    y_val = batch[1].to('cuda').float()[0]\n",
        "                    y_out = model(X_val, weight, length)[0]\n",
        "                else:\n",
        "                    y_val = batch[1].to('cuda').float()[0]\n",
        "                    y_out = model(X_val)[0]\n",
        "\n",
        "                val_loss = criterion(y_out, y_val)\n",
        "                validation_running_loss += val_loss.item()\n",
        "\n",
        "            print(\"================================================================================\")\n",
        "            print(\"Epoch {} completed\".format(e + 1))\n",
        "            train_epoch_loss = train_running_loss / len(train_loader)\n",
        "            validation_epoch_loss = validation_running_loss / len(test_loader)\n",
        "            print(\"Average train loss is {}: \".format(train_epoch_loss))\n",
        "            print(\"Average validation loss is {}\".format(validation_epoch_loss))\n",
        "            print(\"================================================================================\")\n",
        "            train_running_loss_history.append(train_epoch_loss)\n",
        "            validation_running_loss_history.append(validation_epoch_loss)\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if adaptive:\n",
        "            scheduler.step()\n",
        "\n",
        "    return train_running_loss_history, validation_running_loss_history"
      ],
      "metadata": {
        "id": "ALUep79jyBCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last cell defines our statistics functions which will be used extensively later on to quantify our model truths."
      ],
      "metadata": {
        "id": "kN6g9THL2FOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_error(y_true, y_pred):\n",
        "    return torch.mean(torch.abs(y_true - y_pred))\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return torch.mean((y_true - y_pred)**2)\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    return math.sqrt(torch.mean((y_true - y_pred)**2))\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return torch.mean(torch.abs((y_true - y_pred) / y_true)) * 100\n",
        "def r_squared(y_true, y_pred):\n",
        "    ss_residual = torch.sum((y_true - y_pred)**2)\n",
        "    ss_total = torch.sum((y_true - torch.mean(y_true))**2)\n",
        "    r2 = 1 - (ss_residual / ss_total)\n",
        "    return r2"
      ],
      "metadata": {
        "id": "j2785aC02DvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loader (Grad-CAM)\n",
        "\n",
        "We standardize inputs and enforce 4D (N,C,H,W) tensors for CAM. Images are resized to 256×256, converted to tensors, and normalized with (0.5, 0.5, 0.5). We instantiate PatientDataset with no augmentation/denoising, a single region \"Q\" (Quadriceps), number_of_image=1, crop=[1,1,1], and output=\"FM\". Any None samples are filtered out. The dataset is split 90/10 via random_split into train/test; both loaders use batch_size=1 and shuffle=True. A one-batch sanity check unpacks (images, label, weight, length) and, if needed, stacks the image list to ensure a 4D tensor (expected shape (1, 3, 256, 256)), confirming labels/auxiliary metadata are correctly retrieved for training and later Grad-CAM visualization."
      ],
      "metadata": {
        "id": "yEEndV8NyrNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define normalization factors\n",
        "normalization_factors = (0.5, 0.5, 0.5)\n",
        "\n",
        "# Define transformation\n",
        "tx_X = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(normalization_factors, normalization_factors)\n",
        "])\n",
        "\n",
        "# Create the dataset with the required parameters\n",
        "dataset = PatientDataset(\n",
        "    transform=tx_X,\n",
        "    augmented_dataset=False,\n",
        "    augment=0,\n",
        "    threshold=False,\n",
        "    speckle=False,\n",
        "    despeckle=False,\n",
        "    region=\"Q\",  # Choose one region \"B\", \"A\", or \"Q\"\n",
        "    number_of_image=1,       # Only one image per region\n",
        "    crop=[1, 1, 1],          # Crop percentages for each region\n",
        "    output=\"FM\"              # Output label type \"FM\" or \"FFM\"\n",
        ")\n",
        "\n",
        "dataset = [item for item in dataset if item is not None]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_size = int(0.9 * len(dataset))  # 90% for training\n",
        "test_size = len(dataset) - train_size  # Remaining 10% for testing\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 1\n",
        "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
        "\n",
        "# Print the number of batches in each DataLoader\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of testing batches: {len(test_loader)}\")\n",
        "\n",
        "# Fetch a single batch from the train loader to verify\n",
        "first_batch = next(iter(train_loader))\n",
        "\n",
        "# Unpack the batch\n",
        "first_batch_images, label_FM, weight, length = first_batch\n",
        "\n",
        "# Convert the list to a tensor if necessary\n",
        "if isinstance(first_batch_images, list):\n",
        "    first_batch_images = torch.stack(first_batch_images)\n",
        "\n",
        "# Print the shapes to verify\n",
        "print(f\"Shape of first batch images: {first_batch_images.shape}\")  # Expected: (1, 3, 256, 256)\n",
        "print(f\"Label FM: {label_FM}\")\n",
        "print(f\"Weight: {weight}\")\n",
        "print(f\"Length: {length}\")"
      ],
      "metadata": {
        "id": "CDyBrrwRyulo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80cb38b4-9a33-4c9c-ddb1-dd51bda1eddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training batches: 52\n",
            "Number of testing batches: 6\n",
            "Shape of first batch images: torch.Size([1, 3, 256, 256])\n",
            "Label FM: tensor([[0.4044]])\n",
            "Weight: tensor([[2.5007]])\n",
            "Length: tensor([[46.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNet Model with 4D Compatibility for Grad-CAM\n",
        "\n",
        "The following section defines the UNet architecture adapted for Grad-CAM visualization in body composition regression.\n",
        "Unlike a standard segmentation UNet, this implementation preserves a 4D feature map for CAM extraction, applies dropout for regularization, and includes fully connected layers to reduce the output map to a scalar prediction (FM or FFM)."
      ],
      "metadata": {
        "id": "iifT4lfPveQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    '''performs two convolution with batch normalization and LeakyReLU activation'''\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.LeakyReLU(0.1, inplace=False),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.LeakyReLU(0.1, inplace=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class InConv(nn.Module):\n",
        "    '''input convolution block to raw images'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(InConv, self).__init__()\n",
        "        self.conv = DoubleConv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class Down(nn.Module):\n",
        "    '''downsampling with maxpooling and double_conv'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(Down, self).__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mpconv(x)\n",
        "        return x\n",
        "\n",
        "class Up(nn.Module):\n",
        "    '''upsampling and skip connections'''\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(Up, self).__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diff1 = x2.size()[2] - x1.size()[2]\n",
        "        diff2 = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, (diff1 // 2, diff1 - diff1 // 2, diff2 // 2, diff2 - diff2 // 2))\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    ''' the Unet class and architecture'''\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        '''Encoder path, contracting'''\n",
        "        self.inc = InConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 512)\n",
        "        '''Decoder path, expanding'''\n",
        "        self.up1 = Up(1024, 256)\n",
        "        self.up2 = Up(512, 128)\n",
        "        self.up3 = Up(256, 64)\n",
        "        self.up4 = Up(128, 64)\n",
        "        '''output and regression layers'''\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(65536, 1)\n",
        "        self.linear2 = nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''forward pass of the Unet model above'''\n",
        "        x = x.float()\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "b2Nx1QCzyhy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Instantiated the model and sending to GPU\n",
        "MODEL = \"UNet\"\n",
        "model = UNet(3, 1).cuda()"
      ],
      "metadata": {
        "id": "MzHZuXV7yo4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "tERJi4DKyrvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "CRITERION = \"MSE\" # other options include MAE, MAPE, Custom\n",
        "WL = False # Include weight + length metadata (True/False)\n",
        "EPOCHS = 90\n",
        "LR = 0.001  #0.001 for UNet\n",
        "ADAPTIVE = True # Use adaptive learning rate scheduling\n",
        "train_history, validation_history = training(EPOCHS, CRITERION, LR, model, MODEL, ADAPTIVE, WL)"
      ],
      "metadata": {
        "id": "7wAdh54Zyy3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "218b3e17-5f7d-40f6-865a-e48a38526726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1 Batch:  0 Current Loss:  0.14244669675827026\n",
            "Epoch:  1 Batch:  5 Current Loss:  222.01121520996094\n",
            "Epoch:  1 Batch:  10 Current Loss:  69.27972412109375\n",
            "Epoch:  1 Batch:  15 Current Loss:  8.507641792297363\n",
            "Epoch:  1 Batch:  20 Current Loss:  8.213659286499023\n",
            "Epoch:  1 Batch:  25 Current Loss:  3.5066912174224854\n",
            "Epoch:  1 Batch:  30 Current Loss:  0.04494277387857437\n",
            "Epoch:  1 Batch:  35 Current Loss:  0.13024316728115082\n",
            "Epoch:  1 Batch:  40 Current Loss:  1.7445549964904785\n",
            "Epoch:  1 Batch:  45 Current Loss:  0.6128560304641724\n",
            "Epoch:  1 Batch:  50 Current Loss:  0.5151197910308838\n",
            "================================================================================\n",
            "Epoch 1 completed\n",
            "Average train loss is 36.00118534802683: \n",
            "Average validation loss is 0.28266832903803635\n",
            "================================================================================\n",
            "Epoch:  2 Batch:  0 Current Loss:  3.8470723628997803\n",
            "Epoch:  2 Batch:  5 Current Loss:  1.368618130683899\n",
            "Epoch:  2 Batch:  10 Current Loss:  0.5557239651679993\n",
            "Epoch:  2 Batch:  15 Current Loss:  0.3017197549343109\n",
            "Epoch:  2 Batch:  20 Current Loss:  1.260148286819458\n",
            "Epoch:  2 Batch:  25 Current Loss:  0.042137324810028076\n",
            "Epoch:  2 Batch:  30 Current Loss:  0.049410928040742874\n",
            "Epoch:  2 Batch:  35 Current Loss:  2.078540086746216\n",
            "Epoch:  2 Batch:  40 Current Loss:  0.31978312134742737\n",
            "Epoch:  2 Batch:  45 Current Loss:  0.6863820552825928\n",
            "Epoch:  2 Batch:  50 Current Loss:  0.42873039841651917\n",
            "================================================================================\n",
            "Epoch 2 completed\n",
            "Average train loss is 0.6364807142924293: \n",
            "Average validation loss is 0.11949382689393435\n",
            "================================================================================\n",
            "Epoch:  3 Batch:  0 Current Loss:  0.029041580855846405\n",
            "Epoch:  3 Batch:  5 Current Loss:  0.05773863196372986\n",
            "Epoch:  3 Batch:  10 Current Loss:  0.006803924683481455\n",
            "Epoch:  3 Batch:  15 Current Loss:  0.18471331894397736\n",
            "Epoch:  3 Batch:  20 Current Loss:  0.0031984439119696617\n",
            "Epoch:  3 Batch:  25 Current Loss:  0.4028353989124298\n",
            "Epoch:  3 Batch:  30 Current Loss:  0.15612386167049408\n",
            "Epoch:  3 Batch:  35 Current Loss:  0.43263059854507446\n",
            "Epoch:  3 Batch:  40 Current Loss:  0.26421430706977844\n",
            "Epoch:  3 Batch:  45 Current Loss:  0.0058015440590679646\n",
            "Epoch:  3 Batch:  50 Current Loss:  0.009081780910491943\n",
            "================================================================================\n",
            "Epoch 3 completed\n",
            "Average train loss is 0.18024723216792785: \n",
            "Average validation loss is 0.23647069519696137\n",
            "================================================================================\n",
            "Epoch:  4 Batch:  0 Current Loss:  0.04784291982650757\n",
            "Epoch:  4 Batch:  5 Current Loss:  0.4012804627418518\n",
            "Epoch:  4 Batch:  10 Current Loss:  0.6861213445663452\n",
            "Epoch:  4 Batch:  15 Current Loss:  0.651564359664917\n",
            "Epoch:  4 Batch:  20 Current Loss:  0.05196237564086914\n",
            "Epoch:  4 Batch:  25 Current Loss:  0.022990114986896515\n",
            "Epoch:  4 Batch:  30 Current Loss:  0.02850322425365448\n",
            "Epoch:  4 Batch:  35 Current Loss:  0.035435792058706284\n",
            "Epoch:  4 Batch:  40 Current Loss:  0.12567633390426636\n",
            "Epoch:  4 Batch:  45 Current Loss:  0.03544453904032707\n",
            "Epoch:  4 Batch:  50 Current Loss:  0.02281586453318596\n",
            "================================================================================\n",
            "Epoch 4 completed\n",
            "Average train loss is 0.27317089442826376: \n",
            "Average validation loss is 0.05135632288268729\n",
            "================================================================================\n",
            "Epoch:  5 Batch:  0 Current Loss:  0.021698467433452606\n",
            "Epoch:  5 Batch:  5 Current Loss:  0.0651487335562706\n",
            "Epoch:  5 Batch:  10 Current Loss:  0.0028934190049767494\n",
            "Epoch:  5 Batch:  15 Current Loss:  0.3824404180049896\n",
            "Epoch:  5 Batch:  20 Current Loss:  0.17272529006004333\n",
            "Epoch:  5 Batch:  25 Current Loss:  0.05190163105726242\n",
            "Epoch:  5 Batch:  30 Current Loss:  0.002520740730687976\n",
            "Epoch:  5 Batch:  35 Current Loss:  0.0024587654042989016\n",
            "Epoch:  5 Batch:  40 Current Loss:  0.04616445675492287\n",
            "Epoch:  5 Batch:  45 Current Loss:  0.08382603526115417\n",
            "Epoch:  5 Batch:  50 Current Loss:  0.14639152586460114\n",
            "================================================================================\n",
            "Epoch 5 completed\n",
            "Average train loss is 0.10357606770436362: \n",
            "Average validation loss is 0.06337935663759708\n",
            "================================================================================\n",
            "Epoch:  6 Batch:  0 Current Loss:  6.83162753034594e-08\n",
            "Epoch:  6 Batch:  5 Current Loss:  0.09041433036327362\n",
            "Epoch:  6 Batch:  10 Current Loss:  0.08481815457344055\n",
            "Epoch:  6 Batch:  15 Current Loss:  0.05955657362937927\n",
            "Epoch:  6 Batch:  20 Current Loss:  9.45571628108155e-07\n",
            "Epoch:  6 Batch:  25 Current Loss:  0.06975162029266357\n",
            "Epoch:  6 Batch:  30 Current Loss:  0.24089957773685455\n",
            "Epoch:  6 Batch:  35 Current Loss:  0.026460157707333565\n",
            "Epoch:  6 Batch:  40 Current Loss:  0.250756174325943\n",
            "Epoch:  6 Batch:  45 Current Loss:  0.06728669255971909\n",
            "Epoch:  6 Batch:  50 Current Loss:  0.007415429223328829\n",
            "================================================================================\n",
            "Epoch 6 completed\n",
            "Average train loss is 0.08069913255563417: \n",
            "Average validation loss is 0.19371793481210867\n",
            "================================================================================\n",
            "Epoch:  7 Batch:  0 Current Loss:  0.04197512939572334\n",
            "Epoch:  7 Batch:  5 Current Loss:  0.052864283323287964\n",
            "Epoch:  7 Batch:  10 Current Loss:  0.01467499602586031\n",
            "Epoch:  7 Batch:  15 Current Loss:  0.03213225677609444\n",
            "Epoch:  7 Batch:  20 Current Loss:  0.0019308237824589014\n",
            "Epoch:  7 Batch:  25 Current Loss:  0.07263493537902832\n",
            "Epoch:  7 Batch:  30 Current Loss:  0.0008761508506722748\n",
            "Epoch:  7 Batch:  35 Current Loss:  0.01917611062526703\n",
            "Epoch:  7 Batch:  40 Current Loss:  0.008214185014367104\n",
            "Epoch:  7 Batch:  45 Current Loss:  0.0036009964533150196\n",
            "Epoch:  7 Batch:  50 Current Loss:  0.018248755484819412\n",
            "================================================================================\n",
            "Epoch 7 completed\n",
            "Average train loss is 0.04057188301311608: \n",
            "Average validation loss is 0.05570520659481796\n",
            "================================================================================\n",
            "Epoch:  8 Batch:  0 Current Loss:  0.060357172042131424\n",
            "Epoch:  8 Batch:  5 Current Loss:  9.993127605412155e-05\n",
            "Epoch:  8 Batch:  10 Current Loss:  0.04452366381883621\n",
            "Epoch:  8 Batch:  15 Current Loss:  0.00911104679107666\n",
            "Epoch:  8 Batch:  20 Current Loss:  0.0014264403143897653\n",
            "Epoch:  8 Batch:  25 Current Loss:  0.05373917892575264\n",
            "Epoch:  8 Batch:  30 Current Loss:  0.005210088565945625\n",
            "Epoch:  8 Batch:  35 Current Loss:  0.0005980300484225154\n",
            "Epoch:  8 Batch:  40 Current Loss:  1.3486621355696116e-05\n",
            "Epoch:  8 Batch:  45 Current Loss:  0.08352722227573395\n",
            "Epoch:  8 Batch:  50 Current Loss:  0.0693148523569107\n",
            "================================================================================\n",
            "Epoch 8 completed\n",
            "Average train loss is 0.026614058185854307: \n",
            "Average validation loss is 0.04712261015235223\n",
            "================================================================================\n",
            "Epoch:  9 Batch:  0 Current Loss:  0.011298701167106628\n",
            "Epoch:  9 Batch:  5 Current Loss:  0.005865651648491621\n",
            "Epoch:  9 Batch:  10 Current Loss:  0.04447429999709129\n",
            "Epoch:  9 Batch:  15 Current Loss:  0.002545528579503298\n",
            "Epoch:  9 Batch:  20 Current Loss:  0.024982817471027374\n",
            "Epoch:  9 Batch:  25 Current Loss:  0.00921400636434555\n",
            "Epoch:  9 Batch:  30 Current Loss:  0.10077226161956787\n",
            "Epoch:  9 Batch:  35 Current Loss:  0.2163093239068985\n",
            "Epoch:  9 Batch:  40 Current Loss:  0.08948613703250885\n",
            "Epoch:  9 Batch:  45 Current Loss:  0.00014331752026919276\n",
            "Epoch:  9 Batch:  50 Current Loss:  0.0024965573102235794\n",
            "================================================================================\n",
            "Epoch 9 completed\n",
            "Average train loss is 0.045412038455162056: \n",
            "Average validation loss is 0.06604569078384277\n",
            "================================================================================\n",
            "Epoch:  10 Batch:  0 Current Loss:  0.008795682340860367\n",
            "Epoch:  10 Batch:  5 Current Loss:  3.304911899704166e-10\n",
            "Epoch:  10 Batch:  10 Current Loss:  0.01541042048484087\n",
            "Epoch:  10 Batch:  15 Current Loss:  0.031324394047260284\n",
            "Epoch:  10 Batch:  20 Current Loss:  0.008334413170814514\n",
            "Epoch:  10 Batch:  25 Current Loss:  0.030421702191233635\n",
            "Epoch:  10 Batch:  30 Current Loss:  0.029225628823041916\n",
            "Epoch:  10 Batch:  35 Current Loss:  0.039154812693595886\n",
            "Epoch:  10 Batch:  40 Current Loss:  7.8657700214535e-05\n",
            "Epoch:  10 Batch:  45 Current Loss:  0.0015767018776386976\n",
            "Epoch:  10 Batch:  50 Current Loss:  0.00138364068698138\n",
            "================================================================================\n",
            "Epoch 10 completed\n",
            "Average train loss is 0.019884563650965043: \n",
            "Average validation loss is 0.09190858451173274\n",
            "================================================================================\n",
            "Epoch:  11 Batch:  0 Current Loss:  0.0018312132451683283\n",
            "Epoch:  11 Batch:  5 Current Loss:  0.0025450391694903374\n",
            "Epoch:  11 Batch:  10 Current Loss:  0.015472940169274807\n",
            "Epoch:  11 Batch:  15 Current Loss:  0.00012813891225960106\n",
            "Epoch:  11 Batch:  20 Current Loss:  0.003630484454333782\n",
            "Epoch:  11 Batch:  25 Current Loss:  0.00010877574823098257\n",
            "Epoch:  11 Batch:  30 Current Loss:  0.0019405827624723315\n",
            "Epoch:  11 Batch:  35 Current Loss:  0.0009612191934138536\n",
            "Epoch:  11 Batch:  40 Current Loss:  0.008758347481489182\n",
            "Epoch:  11 Batch:  45 Current Loss:  0.07274546474218369\n",
            "Epoch:  11 Batch:  50 Current Loss:  0.027329277247190475\n",
            "================================================================================\n",
            "Epoch 11 completed\n",
            "Average train loss is 0.013289583798819667: \n",
            "Average validation loss is 0.06971528972887124\n",
            "================================================================================\n",
            "Epoch:  12 Batch:  0 Current Loss:  0.017729587852954865\n",
            "Epoch:  12 Batch:  5 Current Loss:  0.0042172642424702644\n",
            "Epoch:  12 Batch:  10 Current Loss:  0.021001065149903297\n",
            "Epoch:  12 Batch:  15 Current Loss:  0.01850920170545578\n",
            "Epoch:  12 Batch:  20 Current Loss:  0.0008937667007558048\n",
            "Epoch:  12 Batch:  25 Current Loss:  0.01111315842717886\n",
            "Epoch:  12 Batch:  30 Current Loss:  0.011942630633711815\n",
            "Epoch:  12 Batch:  35 Current Loss:  9.420654532732442e-05\n",
            "Epoch:  12 Batch:  40 Current Loss:  0.006945506669580936\n",
            "Epoch:  12 Batch:  45 Current Loss:  0.046240631490945816\n",
            "Epoch:  12 Batch:  50 Current Loss:  0.13370917737483978\n",
            "================================================================================\n",
            "Epoch 12 completed\n",
            "Average train loss is 0.01975460663774916: \n",
            "Average validation loss is 0.06466737466871564\n",
            "================================================================================\n",
            "Epoch:  13 Batch:  0 Current Loss:  0.03905320540070534\n",
            "Epoch:  13 Batch:  5 Current Loss:  0.0037430957891047\n",
            "Epoch:  13 Batch:  10 Current Loss:  0.0006615218590013683\n",
            "Epoch:  13 Batch:  15 Current Loss:  0.047544967383146286\n",
            "Epoch:  13 Batch:  20 Current Loss:  0.02733665704727173\n",
            "Epoch:  13 Batch:  25 Current Loss:  0.018492888659238815\n",
            "Epoch:  13 Batch:  30 Current Loss:  0.008278531022369862\n",
            "Epoch:  13 Batch:  35 Current Loss:  0.004356890916824341\n",
            "Epoch:  13 Batch:  40 Current Loss:  0.029079172760248184\n",
            "Epoch:  13 Batch:  45 Current Loss:  0.0020648711360991\n",
            "Epoch:  13 Batch:  50 Current Loss:  0.0021571419201791286\n",
            "================================================================================\n",
            "Epoch 13 completed\n",
            "Average train loss is 0.030397723103585753: \n",
            "Average validation loss is 0.07957212545443326\n",
            "================================================================================\n",
            "Epoch:  14 Batch:  0 Current Loss:  0.0030662580393254757\n",
            "Epoch:  14 Batch:  5 Current Loss:  0.001510263537056744\n",
            "Epoch:  14 Batch:  10 Current Loss:  0.008259976282715797\n",
            "Epoch:  14 Batch:  15 Current Loss:  1.1872665481860167e-06\n",
            "Epoch:  14 Batch:  20 Current Loss:  0.030352633446455002\n",
            "Epoch:  14 Batch:  25 Current Loss:  0.0018950868397951126\n",
            "Epoch:  14 Batch:  30 Current Loss:  0.008342773653566837\n",
            "Epoch:  14 Batch:  35 Current Loss:  0.04372578486800194\n",
            "Epoch:  14 Batch:  40 Current Loss:  0.0631616860628128\n",
            "Epoch:  14 Batch:  45 Current Loss:  0.013278734870254993\n",
            "Epoch:  14 Batch:  50 Current Loss:  0.05066652595996857\n",
            "================================================================================\n",
            "Epoch 14 completed\n",
            "Average train loss is 0.016860649492748243: \n",
            "Average validation loss is 0.06202659886912443\n",
            "================================================================================\n",
            "Epoch:  15 Batch:  0 Current Loss:  0.01569337397813797\n",
            "Epoch:  15 Batch:  5 Current Loss:  0.002226998331025243\n",
            "Epoch:  15 Batch:  10 Current Loss:  0.0480538085103035\n",
            "Epoch:  15 Batch:  15 Current Loss:  0.024863163009285927\n",
            "Epoch:  15 Batch:  20 Current Loss:  0.014186181128025055\n",
            "Epoch:  15 Batch:  25 Current Loss:  0.024189680814743042\n",
            "Epoch:  15 Batch:  30 Current Loss:  0.00809583067893982\n",
            "Epoch:  15 Batch:  35 Current Loss:  6.53905954095535e-06\n",
            "Epoch:  15 Batch:  40 Current Loss:  0.020119957625865936\n",
            "Epoch:  15 Batch:  45 Current Loss:  0.04663429036736488\n",
            "Epoch:  15 Batch:  50 Current Loss:  0.0018512931419536471\n",
            "================================================================================\n",
            "Epoch 15 completed\n",
            "Average train loss is 0.016525552520936168: \n",
            "Average validation loss is 0.08679208004226287\n",
            "================================================================================\n",
            "Epoch:  16 Batch:  0 Current Loss:  0.011824440211057663\n",
            "Epoch:  16 Batch:  5 Current Loss:  0.011880564503371716\n",
            "Epoch:  16 Batch:  10 Current Loss:  0.005546100437641144\n",
            "Epoch:  16 Batch:  15 Current Loss:  0.017860786989331245\n",
            "Epoch:  16 Batch:  20 Current Loss:  0.036010999232530594\n",
            "Epoch:  16 Batch:  25 Current Loss:  0.0007155496277846396\n",
            "Epoch:  16 Batch:  30 Current Loss:  0.008177192881703377\n",
            "Epoch:  16 Batch:  35 Current Loss:  0.0006693869363516569\n",
            "Epoch:  16 Batch:  40 Current Loss:  0.015796562656760216\n",
            "Epoch:  16 Batch:  45 Current Loss:  0.04966646432876587\n",
            "Epoch:  16 Batch:  50 Current Loss:  0.0018063934985548258\n",
            "================================================================================\n",
            "Epoch 16 completed\n",
            "Average train loss is 0.011438567972119857: \n",
            "Average validation loss is 0.07535537648557995\n",
            "================================================================================\n",
            "Epoch:  17 Batch:  0 Current Loss:  0.02037946693599224\n",
            "Epoch:  17 Batch:  5 Current Loss:  0.015562684275209904\n",
            "Epoch:  17 Batch:  10 Current Loss:  0.0017029093578457832\n",
            "Epoch:  17 Batch:  15 Current Loss:  0.00019657198572531343\n",
            "Epoch:  17 Batch:  20 Current Loss:  0.011073143221437931\n",
            "Epoch:  17 Batch:  25 Current Loss:  2.739073352131527e-05\n",
            "Epoch:  17 Batch:  30 Current Loss:  0.008547755889594555\n",
            "Epoch:  17 Batch:  35 Current Loss:  0.022100815549492836\n",
            "Epoch:  17 Batch:  40 Current Loss:  0.011247326619923115\n",
            "Epoch:  17 Batch:  45 Current Loss:  0.0035508202854543924\n",
            "Epoch:  17 Batch:  50 Current Loss:  2.063023657683516e-06\n",
            "================================================================================\n",
            "Epoch 17 completed\n",
            "Average train loss is 0.007796533772665708: \n",
            "Average validation loss is 0.06997538885722558\n",
            "================================================================================\n",
            "Epoch:  18 Batch:  0 Current Loss:  0.009279935620725155\n",
            "Epoch:  18 Batch:  5 Current Loss:  0.009540705010294914\n",
            "Epoch:  18 Batch:  10 Current Loss:  0.0019405055791139603\n",
            "Epoch:  18 Batch:  15 Current Loss:  0.013503899797797203\n",
            "Epoch:  18 Batch:  20 Current Loss:  0.0008413707837462425\n",
            "Epoch:  18 Batch:  25 Current Loss:  0.0012806948507204652\n",
            "Epoch:  18 Batch:  30 Current Loss:  0.02528698369860649\n",
            "Epoch:  18 Batch:  35 Current Loss:  0.022897379472851753\n",
            "Epoch:  18 Batch:  40 Current Loss:  0.006433495786041021\n",
            "Epoch:  18 Batch:  45 Current Loss:  0.003099683206528425\n",
            "Epoch:  18 Batch:  50 Current Loss:  0.0016315554967150092\n",
            "================================================================================\n",
            "Epoch 18 completed\n",
            "Average train loss is 0.01215379708747395: \n",
            "Average validation loss is 0.07925638027760822\n",
            "================================================================================\n",
            "Epoch:  19 Batch:  0 Current Loss:  7.219647523015738e-05\n",
            "Epoch:  19 Batch:  5 Current Loss:  0.01896570809185505\n",
            "Epoch:  19 Batch:  10 Current Loss:  0.0029674016404896975\n",
            "Epoch:  19 Batch:  15 Current Loss:  0.002015054691582918\n",
            "Epoch:  19 Batch:  20 Current Loss:  0.0010857917368412018\n",
            "Epoch:  19 Batch:  25 Current Loss:  0.00012720008089672774\n",
            "Epoch:  19 Batch:  30 Current Loss:  0.005687613505870104\n",
            "Epoch:  19 Batch:  35 Current Loss:  0.0007289828499779105\n",
            "Epoch:  19 Batch:  40 Current Loss:  4.490849460125901e-05\n",
            "Epoch:  19 Batch:  45 Current Loss:  0.015355614945292473\n",
            "Epoch:  19 Batch:  50 Current Loss:  0.0016951174475252628\n",
            "================================================================================\n",
            "Epoch 19 completed\n",
            "Average train loss is 0.00501765691044749: \n",
            "Average validation loss is 0.06434399246548612\n",
            "================================================================================\n",
            "Epoch:  20 Batch:  0 Current Loss:  0.0018147287191823125\n",
            "Epoch:  20 Batch:  5 Current Loss:  0.0010814496781677008\n",
            "Epoch:  20 Batch:  10 Current Loss:  0.0010854520369321108\n",
            "Epoch:  20 Batch:  15 Current Loss:  0.0010222262935712934\n",
            "Epoch:  20 Batch:  20 Current Loss:  0.0045272973366081715\n",
            "Epoch:  20 Batch:  25 Current Loss:  0.0011674001580104232\n",
            "Epoch:  20 Batch:  30 Current Loss:  0.0011272304691374302\n",
            "Epoch:  20 Batch:  35 Current Loss:  0.002839345484972\n",
            "Epoch:  20 Batch:  40 Current Loss:  0.010083242319524288\n",
            "Epoch:  20 Batch:  45 Current Loss:  0.007411960046738386\n",
            "Epoch:  20 Batch:  50 Current Loss:  0.0026150643825531006\n",
            "================================================================================\n",
            "Epoch 20 completed\n",
            "Average train loss is 0.007085061775571743: \n",
            "Average validation loss is 0.09034662662694852\n",
            "================================================================================\n",
            "Epoch:  21 Batch:  0 Current Loss:  0.002282771049067378\n",
            "Epoch:  21 Batch:  5 Current Loss:  0.00021031545475125313\n",
            "Epoch:  21 Batch:  10 Current Loss:  0.017857830971479416\n",
            "Epoch:  21 Batch:  15 Current Loss:  0.00020462649990804493\n",
            "Epoch:  21 Batch:  20 Current Loss:  0.0033270171843469143\n",
            "Epoch:  21 Batch:  25 Current Loss:  4.3562653445405886e-05\n",
            "Epoch:  21 Batch:  30 Current Loss:  0.006873208563774824\n",
            "Epoch:  21 Batch:  35 Current Loss:  0.0215279720723629\n",
            "Epoch:  21 Batch:  40 Current Loss:  0.0004890801501460373\n",
            "Epoch:  21 Batch:  45 Current Loss:  0.01018840353935957\n",
            "Epoch:  21 Batch:  50 Current Loss:  0.00428807083517313\n",
            "================================================================================\n",
            "Epoch 21 completed\n",
            "Average train loss is 0.007123108617494067: \n",
            "Average validation loss is 0.06742436376710732\n",
            "================================================================================\n",
            "Epoch:  22 Batch:  0 Current Loss:  0.00014749083493370563\n",
            "Epoch:  22 Batch:  5 Current Loss:  0.0008573903469368815\n",
            "Epoch:  22 Batch:  10 Current Loss:  0.0050058490596711636\n",
            "Epoch:  22 Batch:  15 Current Loss:  0.0007889685803093016\n",
            "Epoch:  22 Batch:  20 Current Loss:  0.002838037209585309\n",
            "Epoch:  22 Batch:  25 Current Loss:  0.0006372714415192604\n",
            "Epoch:  22 Batch:  30 Current Loss:  0.00017356598982587457\n",
            "Epoch:  22 Batch:  35 Current Loss:  0.0021488794591277838\n",
            "Epoch:  22 Batch:  40 Current Loss:  0.0007465369999408722\n",
            "Epoch:  22 Batch:  45 Current Loss:  0.000978985452093184\n",
            "Epoch:  22 Batch:  50 Current Loss:  0.00984764564782381\n",
            "================================================================================\n",
            "Epoch 22 completed\n",
            "Average train loss is 0.002976362012095006: \n",
            "Average validation loss is 0.06665954443936546\n",
            "================================================================================\n",
            "Epoch:  23 Batch:  0 Current Loss:  0.0028474803548306227\n",
            "Epoch:  23 Batch:  5 Current Loss:  0.0009577961172908545\n",
            "Epoch:  23 Batch:  10 Current Loss:  0.008321059867739677\n",
            "Epoch:  23 Batch:  15 Current Loss:  0.0005661436007358134\n",
            "Epoch:  23 Batch:  20 Current Loss:  1.2880845133622643e-05\n",
            "Epoch:  23 Batch:  25 Current Loss:  5.883950052520959e-06\n",
            "Epoch:  23 Batch:  30 Current Loss:  3.4522597616160056e-06\n",
            "Epoch:  23 Batch:  35 Current Loss:  0.00022972603619564325\n",
            "Epoch:  23 Batch:  40 Current Loss:  0.007237546145915985\n",
            "Epoch:  23 Batch:  45 Current Loss:  0.0032375145237892866\n",
            "Epoch:  23 Batch:  50 Current Loss:  0.0035942518152296543\n",
            "================================================================================\n",
            "Epoch 23 completed\n",
            "Average train loss is 0.002421265400738111: \n",
            "Average validation loss is 0.06912143365480006\n",
            "================================================================================\n",
            "Epoch:  24 Batch:  0 Current Loss:  0.00016686921298969537\n",
            "Epoch:  24 Batch:  5 Current Loss:  0.0006451596855185926\n",
            "Epoch:  24 Batch:  10 Current Loss:  0.0008855478372424841\n",
            "Epoch:  24 Batch:  15 Current Loss:  1.2050027180521283e-05\n",
            "Epoch:  24 Batch:  20 Current Loss:  0.0003706510760821402\n",
            "Epoch:  24 Batch:  25 Current Loss:  0.0014995485544204712\n",
            "Epoch:  24 Batch:  30 Current Loss:  1.087303644453641e-06\n",
            "Epoch:  24 Batch:  35 Current Loss:  2.3337708626058884e-05\n",
            "Epoch:  24 Batch:  40 Current Loss:  0.0039741951040923595\n",
            "Epoch:  24 Batch:  45 Current Loss:  0.001746353809721768\n",
            "Epoch:  24 Batch:  50 Current Loss:  0.0007276565884239972\n",
            "================================================================================\n",
            "Epoch 24 completed\n",
            "Average train loss is 0.002926286078875889: \n",
            "Average validation loss is 0.07829293220614393\n",
            "================================================================================\n",
            "Epoch:  25 Batch:  0 Current Loss:  0.0004223115975037217\n",
            "Epoch:  25 Batch:  5 Current Loss:  0.0026740229222923517\n",
            "Epoch:  25 Batch:  10 Current Loss:  0.0011151867220178246\n",
            "Epoch:  25 Batch:  15 Current Loss:  0.00010375386773375794\n",
            "Epoch:  25 Batch:  20 Current Loss:  0.00044478120980784297\n",
            "Epoch:  25 Batch:  25 Current Loss:  2.0774521544808522e-05\n",
            "Epoch:  25 Batch:  30 Current Loss:  0.00022199087834451348\n",
            "Epoch:  25 Batch:  35 Current Loss:  0.0014148749178275466\n",
            "Epoch:  25 Batch:  40 Current Loss:  0.0010166378924623132\n",
            "Epoch:  25 Batch:  45 Current Loss:  2.576735096226912e-05\n",
            "Epoch:  25 Batch:  50 Current Loss:  0.0001209665642818436\n",
            "================================================================================\n",
            "Epoch 25 completed\n",
            "Average train loss is 0.001434781403038005: \n",
            "Average validation loss is 0.07416583962428074\n",
            "================================================================================\n",
            "Epoch:  26 Batch:  0 Current Loss:  0.0005953372456133366\n",
            "Epoch:  26 Batch:  5 Current Loss:  5.5578006140422076e-05\n",
            "Epoch:  26 Batch:  10 Current Loss:  0.0013934801099821925\n",
            "Epoch:  26 Batch:  15 Current Loss:  0.00017194646352436393\n",
            "Epoch:  26 Batch:  20 Current Loss:  0.0027020114939659834\n",
            "Epoch:  26 Batch:  25 Current Loss:  0.0029822764918208122\n",
            "Epoch:  26 Batch:  30 Current Loss:  0.009743839502334595\n",
            "Epoch:  26 Batch:  35 Current Loss:  0.003087293589487672\n",
            "Epoch:  26 Batch:  40 Current Loss:  0.0013739909045398235\n",
            "Epoch:  26 Batch:  45 Current Loss:  0.00023707211948931217\n",
            "Epoch:  26 Batch:  50 Current Loss:  5.624459390674019e-06\n",
            "================================================================================\n",
            "Epoch 26 completed\n",
            "Average train loss is 0.0013896912673724367: \n",
            "Average validation loss is 0.08163037740935881\n",
            "================================================================================\n",
            "Epoch:  27 Batch:  0 Current Loss:  1.953677201527171e-05\n",
            "Epoch:  27 Batch:  5 Current Loss:  0.0007731561781838536\n",
            "Epoch:  27 Batch:  10 Current Loss:  0.0013748527271673083\n",
            "Epoch:  27 Batch:  15 Current Loss:  5.2522041187330615e-06\n",
            "Epoch:  27 Batch:  20 Current Loss:  0.0011471175821498036\n",
            "Epoch:  27 Batch:  25 Current Loss:  0.0003196317993570119\n",
            "Epoch:  27 Batch:  30 Current Loss:  0.002513585612177849\n",
            "Epoch:  27 Batch:  35 Current Loss:  0.0037425861228257418\n",
            "Epoch:  27 Batch:  40 Current Loss:  0.0007207280723378062\n",
            "Epoch:  27 Batch:  45 Current Loss:  0.0019265995360910892\n",
            "Epoch:  27 Batch:  50 Current Loss:  0.0015633533475920558\n",
            "================================================================================\n",
            "Epoch 27 completed\n",
            "Average train loss is 0.0015130241609339324: \n",
            "Average validation loss is 0.0780595340875152\n",
            "================================================================================\n",
            "Epoch:  28 Batch:  0 Current Loss:  0.00857384130358696\n",
            "Epoch:  28 Batch:  5 Current Loss:  0.00399370864033699\n",
            "Epoch:  28 Batch:  10 Current Loss:  0.0013718967093154788\n",
            "Epoch:  28 Batch:  15 Current Loss:  0.00044968712609261274\n",
            "Epoch:  28 Batch:  20 Current Loss:  0.0041395677253603935\n",
            "Epoch:  28 Batch:  25 Current Loss:  0.00032209386699832976\n",
            "Epoch:  28 Batch:  30 Current Loss:  0.00020263620535843074\n",
            "Epoch:  28 Batch:  35 Current Loss:  0.00012715673074126244\n",
            "Epoch:  28 Batch:  40 Current Loss:  0.0001402279594913125\n",
            "Epoch:  28 Batch:  45 Current Loss:  9.395899724040646e-06\n",
            "Epoch:  28 Batch:  50 Current Loss:  9.331570618087426e-05\n",
            "================================================================================\n",
            "Epoch 28 completed\n",
            "Average train loss is 0.002308290935257691: \n",
            "Average validation loss is 0.06475917451704542\n",
            "================================================================================\n",
            "Epoch:  29 Batch:  0 Current Loss:  0.01430616993457079\n",
            "Epoch:  29 Batch:  5 Current Loss:  0.005255202762782574\n",
            "Epoch:  29 Batch:  10 Current Loss:  0.00650632893666625\n",
            "Epoch:  29 Batch:  15 Current Loss:  0.00040503847412765026\n",
            "Epoch:  29 Batch:  20 Current Loss:  3.450598740073474e-08\n",
            "Epoch:  29 Batch:  25 Current Loss:  0.0030128867365419865\n",
            "Epoch:  29 Batch:  30 Current Loss:  0.000904627435375005\n",
            "Epoch:  29 Batch:  35 Current Loss:  0.0009975016582757235\n",
            "Epoch:  29 Batch:  40 Current Loss:  0.0009332544286735356\n",
            "Epoch:  29 Batch:  45 Current Loss:  0.004720625933259726\n",
            "Epoch:  29 Batch:  50 Current Loss:  0.011260073632001877\n",
            "================================================================================\n",
            "Epoch 29 completed\n",
            "Average train loss is 0.002098825910120894: \n",
            "Average validation loss is 0.07492306921631098\n",
            "================================================================================\n",
            "Epoch:  30 Batch:  0 Current Loss:  0.00016905802476685494\n",
            "Epoch:  30 Batch:  5 Current Loss:  0.00015881720173638314\n",
            "Epoch:  30 Batch:  10 Current Loss:  3.847013067570515e-05\n",
            "Epoch:  30 Batch:  15 Current Loss:  0.0002818996727000922\n",
            "Epoch:  30 Batch:  20 Current Loss:  0.0019756555557250977\n",
            "Epoch:  30 Batch:  25 Current Loss:  0.0007443912327289581\n",
            "Epoch:  30 Batch:  30 Current Loss:  0.002981950994580984\n",
            "Epoch:  30 Batch:  35 Current Loss:  0.0011064736172556877\n",
            "Epoch:  30 Batch:  40 Current Loss:  0.0008087335154414177\n",
            "Epoch:  30 Batch:  45 Current Loss:  0.0017007406568154693\n",
            "Epoch:  30 Batch:  50 Current Loss:  0.002163258846849203\n",
            "================================================================================\n",
            "Epoch 30 completed\n",
            "Average train loss is 0.0016891699691175843: \n",
            "Average validation loss is 0.07072231095905106\n",
            "================================================================================\n",
            "Epoch:  31 Batch:  0 Current Loss:  0.0014321058988571167\n",
            "Epoch:  31 Batch:  5 Current Loss:  0.0076611959375441074\n",
            "Epoch:  31 Batch:  10 Current Loss:  0.0035785501822829247\n",
            "Epoch:  31 Batch:  15 Current Loss:  0.002250208519399166\n",
            "Epoch:  31 Batch:  20 Current Loss:  0.004903103690594435\n",
            "Epoch:  31 Batch:  25 Current Loss:  8.064352005021647e-05\n",
            "Epoch:  31 Batch:  30 Current Loss:  3.9863189158495516e-06\n",
            "Epoch:  31 Batch:  35 Current Loss:  0.000882835010997951\n",
            "Epoch:  31 Batch:  40 Current Loss:  0.003630739403888583\n",
            "Epoch:  31 Batch:  45 Current Loss:  8.194641122827306e-05\n",
            "Epoch:  31 Batch:  50 Current Loss:  0.0008824545075185597\n",
            "================================================================================\n",
            "Epoch 31 completed\n",
            "Average train loss is 0.001887499152592052: \n",
            "Average validation loss is 0.06246220041066408\n",
            "================================================================================\n",
            "Epoch:  32 Batch:  0 Current Loss:  0.0012860700953751802\n",
            "Epoch:  32 Batch:  5 Current Loss:  0.0010428325040265918\n",
            "Epoch:  32 Batch:  10 Current Loss:  1.9911072740796953e-05\n",
            "Epoch:  32 Batch:  15 Current Loss:  0.00021420449775177985\n",
            "Epoch:  32 Batch:  20 Current Loss:  0.003424638882279396\n",
            "Epoch:  32 Batch:  25 Current Loss:  0.004323988687247038\n",
            "Epoch:  32 Batch:  30 Current Loss:  0.005501572508364916\n",
            "Epoch:  32 Batch:  35 Current Loss:  0.0008908040472306311\n",
            "Epoch:  32 Batch:  40 Current Loss:  0.004572485573589802\n",
            "Epoch:  32 Batch:  45 Current Loss:  0.0023168905172497034\n",
            "Epoch:  32 Batch:  50 Current Loss:  0.00025630774325691164\n",
            "================================================================================\n",
            "Epoch 32 completed\n",
            "Average train loss is 0.001498264857664603: \n",
            "Average validation loss is 0.07059094042051584\n",
            "================================================================================\n",
            "Epoch:  33 Batch:  0 Current Loss:  0.0014024801785126328\n",
            "Epoch:  33 Batch:  5 Current Loss:  4.5305778257898055e-06\n",
            "Epoch:  33 Batch:  10 Current Loss:  9.565635991748422e-05\n",
            "Epoch:  33 Batch:  15 Current Loss:  4.2393932631057396e-07\n",
            "Epoch:  33 Batch:  20 Current Loss:  0.00024166762887034565\n",
            "Epoch:  33 Batch:  25 Current Loss:  0.00016173822223208845\n",
            "Epoch:  33 Batch:  30 Current Loss:  0.0001765077468007803\n",
            "Epoch:  33 Batch:  35 Current Loss:  0.0019883050117641687\n",
            "Epoch:  33 Batch:  40 Current Loss:  0.003003164427354932\n",
            "Epoch:  33 Batch:  45 Current Loss:  0.003514598123729229\n",
            "Epoch:  33 Batch:  50 Current Loss:  3.034219162145746e-06\n",
            "================================================================================\n",
            "Epoch 33 completed\n",
            "Average train loss is 0.0008342276911161679: \n",
            "Average validation loss is 0.06600091389069955\n",
            "================================================================================\n",
            "Epoch:  34 Batch:  0 Current Loss:  0.00214094715192914\n",
            "Epoch:  34 Batch:  5 Current Loss:  0.0029524911660701036\n",
            "Epoch:  34 Batch:  10 Current Loss:  2.488436621206347e-06\n",
            "Epoch:  34 Batch:  15 Current Loss:  0.0026747719384729862\n",
            "Epoch:  34 Batch:  20 Current Loss:  0.0026157915126532316\n",
            "Epoch:  34 Batch:  25 Current Loss:  0.0012399675324559212\n",
            "Epoch:  34 Batch:  30 Current Loss:  0.000832997786346823\n",
            "Epoch:  34 Batch:  35 Current Loss:  0.0002993362140841782\n",
            "Epoch:  34 Batch:  40 Current Loss:  0.000639835896436125\n",
            "Epoch:  34 Batch:  45 Current Loss:  3.1685674912296236e-05\n",
            "Epoch:  34 Batch:  50 Current Loss:  0.0010983245447278023\n",
            "================================================================================\n",
            "Epoch 34 completed\n",
            "Average train loss is 0.0010470335703303135: \n",
            "Average validation loss is 0.0596139170229435\n",
            "================================================================================\n",
            "Epoch:  35 Batch:  0 Current Loss:  0.0008269122336059809\n",
            "Epoch:  35 Batch:  5 Current Loss:  0.00496343057602644\n",
            "Epoch:  35 Batch:  10 Current Loss:  0.0006543564377352595\n",
            "Epoch:  35 Batch:  15 Current Loss:  0.00018070245278067887\n",
            "Epoch:  35 Batch:  20 Current Loss:  0.00024513210519216955\n",
            "Epoch:  35 Batch:  25 Current Loss:  0.000539385830052197\n",
            "Epoch:  35 Batch:  30 Current Loss:  0.0004806390788871795\n",
            "Epoch:  35 Batch:  35 Current Loss:  0.001931381761096418\n",
            "Epoch:  35 Batch:  40 Current Loss:  1.1322590580675751e-05\n",
            "Epoch:  35 Batch:  45 Current Loss:  3.481293606455438e-05\n",
            "Epoch:  35 Batch:  50 Current Loss:  3.6215697036823258e-06\n",
            "================================================================================\n",
            "Epoch 35 completed\n",
            "Average train loss is 0.0006179883735109881: \n",
            "Average validation loss is 0.07251899332428972\n",
            "================================================================================\n",
            "Epoch:  36 Batch:  0 Current Loss:  0.00010611292964313179\n",
            "Epoch:  36 Batch:  5 Current Loss:  0.0007543884566985071\n",
            "Epoch:  36 Batch:  10 Current Loss:  0.0007727576303295791\n",
            "Epoch:  36 Batch:  15 Current Loss:  0.0028180077206343412\n",
            "Epoch:  36 Batch:  20 Current Loss:  0.0010484885424375534\n",
            "Epoch:  36 Batch:  25 Current Loss:  0.0005638123257085681\n",
            "Epoch:  36 Batch:  30 Current Loss:  0.0008328421390615404\n",
            "Epoch:  36 Batch:  35 Current Loss:  6.874572136439383e-05\n",
            "Epoch:  36 Batch:  40 Current Loss:  0.0002296948805451393\n",
            "Epoch:  36 Batch:  45 Current Loss:  2.597122517045136e-09\n",
            "Epoch:  36 Batch:  50 Current Loss:  2.981047691719141e-05\n",
            "================================================================================\n",
            "Epoch 36 completed\n",
            "Average train loss is 0.00048471246126094225: \n",
            "Average validation loss is 0.05773626562828819\n",
            "================================================================================\n",
            "Epoch:  37 Batch:  0 Current Loss:  0.003743196139112115\n",
            "Epoch:  37 Batch:  5 Current Loss:  0.0004787748621311039\n",
            "Epoch:  37 Batch:  10 Current Loss:  4.3869211367564276e-05\n",
            "Epoch:  37 Batch:  15 Current Loss:  0.0005875452188774943\n",
            "Epoch:  37 Batch:  20 Current Loss:  0.0009633140289224684\n",
            "Epoch:  37 Batch:  25 Current Loss:  4.580399036058225e-06\n",
            "Epoch:  37 Batch:  30 Current Loss:  6.955512071726844e-05\n",
            "Epoch:  37 Batch:  35 Current Loss:  0.0003686350246425718\n",
            "Epoch:  37 Batch:  40 Current Loss:  2.9056727726128884e-06\n",
            "Epoch:  37 Batch:  45 Current Loss:  2.31500143854646e-06\n",
            "Epoch:  37 Batch:  50 Current Loss:  7.658217509742826e-05\n",
            "================================================================================\n",
            "Epoch 37 completed\n",
            "Average train loss is 0.0004142731945736299: \n",
            "Average validation loss is 0.06200698033596078\n",
            "================================================================================\n",
            "Epoch:  38 Batch:  0 Current Loss:  0.00034819537540897727\n",
            "Epoch:  38 Batch:  5 Current Loss:  0.00023641936422791332\n",
            "Epoch:  38 Batch:  10 Current Loss:  2.1379807435550902e-07\n",
            "Epoch:  38 Batch:  15 Current Loss:  6.356849917210639e-05\n",
            "Epoch:  38 Batch:  20 Current Loss:  0.0004928798298351467\n",
            "Epoch:  38 Batch:  25 Current Loss:  7.029427797533572e-05\n",
            "Epoch:  38 Batch:  30 Current Loss:  0.00017221718735527247\n",
            "Epoch:  38 Batch:  35 Current Loss:  0.0012955976417288184\n",
            "Epoch:  38 Batch:  40 Current Loss:  0.0017952579073607922\n",
            "Epoch:  38 Batch:  45 Current Loss:  0.0004926489782519639\n",
            "Epoch:  38 Batch:  50 Current Loss:  0.0001812244299799204\n",
            "================================================================================\n",
            "Epoch 38 completed\n",
            "Average train loss is 0.0005224939360929809: \n",
            "Average validation loss is 0.07358301516311865\n",
            "================================================================================\n",
            "Epoch:  39 Batch:  0 Current Loss:  1.5395298760267906e-05\n",
            "Epoch:  39 Batch:  5 Current Loss:  6.52595263090916e-05\n",
            "Epoch:  39 Batch:  10 Current Loss:  8.50571097998909e-08\n",
            "Epoch:  39 Batch:  15 Current Loss:  4.4131687900517136e-05\n",
            "Epoch:  39 Batch:  20 Current Loss:  7.685417222091928e-05\n",
            "Epoch:  39 Batch:  25 Current Loss:  0.0008358337800018489\n",
            "Epoch:  39 Batch:  30 Current Loss:  9.826367750065401e-05\n",
            "Epoch:  39 Batch:  35 Current Loss:  4.702427759184502e-05\n",
            "Epoch:  39 Batch:  40 Current Loss:  7.241580897243693e-05\n",
            "Epoch:  39 Batch:  45 Current Loss:  6.296591891441494e-05\n",
            "Epoch:  39 Batch:  50 Current Loss:  7.0324158514267765e-06\n",
            "================================================================================\n",
            "Epoch 39 completed\n",
            "Average train loss is 0.000300918780601413: \n",
            "Average validation loss is 0.07351628209774692\n",
            "================================================================================\n",
            "Epoch:  40 Batch:  0 Current Loss:  0.0009138669120147824\n",
            "Epoch:  40 Batch:  5 Current Loss:  0.00019071222050115466\n",
            "Epoch:  40 Batch:  10 Current Loss:  8.114665433822665e-06\n",
            "Epoch:  40 Batch:  15 Current Loss:  0.0004808056983165443\n",
            "Epoch:  40 Batch:  20 Current Loss:  0.0003394896339159459\n",
            "Epoch:  40 Batch:  25 Current Loss:  7.692342478549108e-05\n",
            "Epoch:  40 Batch:  30 Current Loss:  0.0002233770937891677\n",
            "Epoch:  40 Batch:  35 Current Loss:  1.0385748282715213e-05\n",
            "Epoch:  40 Batch:  40 Current Loss:  0.0002390717127127573\n",
            "Epoch:  40 Batch:  45 Current Loss:  0.00032512881443835795\n",
            "Epoch:  40 Batch:  50 Current Loss:  3.9368871512124315e-05\n",
            "================================================================================\n",
            "Epoch 40 completed\n",
            "Average train loss is 0.00016410515048138362: \n",
            "Average validation loss is 0.07179500410954158\n",
            "================================================================================\n",
            "Epoch:  41 Batch:  0 Current Loss:  0.0003248225839342922\n",
            "Epoch:  41 Batch:  5 Current Loss:  0.00024727522395551205\n",
            "Epoch:  41 Batch:  10 Current Loss:  0.0006735656643286347\n",
            "Epoch:  41 Batch:  15 Current Loss:  0.0008875763160176575\n",
            "Epoch:  41 Batch:  20 Current Loss:  0.0001019830524455756\n",
            "Epoch:  41 Batch:  25 Current Loss:  0.00017474390915594995\n",
            "Epoch:  41 Batch:  30 Current Loss:  0.00011471624748082832\n",
            "Epoch:  41 Batch:  35 Current Loss:  5.20463891007239e-06\n",
            "Epoch:  41 Batch:  40 Current Loss:  4.9319946811010595e-06\n",
            "Epoch:  41 Batch:  45 Current Loss:  0.0006227611447684467\n",
            "Epoch:  41 Batch:  50 Current Loss:  0.00036792384344153106\n",
            "================================================================================\n",
            "Epoch 41 completed\n",
            "Average train loss is 0.0003157945997407716: \n",
            "Average validation loss is 0.06849471231301625\n",
            "================================================================================\n",
            "Epoch:  42 Batch:  0 Current Loss:  0.0001486905530327931\n",
            "Epoch:  42 Batch:  5 Current Loss:  0.00011269353854004294\n",
            "Epoch:  42 Batch:  10 Current Loss:  6.0077123634982854e-05\n",
            "Epoch:  42 Batch:  15 Current Loss:  0.00010606719297356904\n",
            "Epoch:  42 Batch:  20 Current Loss:  7.277860277099535e-06\n",
            "Epoch:  42 Batch:  25 Current Loss:  2.252074409625493e-05\n",
            "Epoch:  42 Batch:  30 Current Loss:  0.00016954624152276665\n",
            "Epoch:  42 Batch:  35 Current Loss:  0.0003805157321039587\n",
            "Epoch:  42 Batch:  40 Current Loss:  6.264447438297793e-05\n",
            "Epoch:  42 Batch:  45 Current Loss:  0.00010761513112811372\n",
            "Epoch:  42 Batch:  50 Current Loss:  0.00013026372471358627\n",
            "================================================================================\n",
            "Epoch 42 completed\n",
            "Average train loss is 0.00017368190784430645: \n",
            "Average validation loss is 0.07317247955749433\n",
            "================================================================================\n",
            "Epoch:  43 Batch:  0 Current Loss:  0.00020463672990445048\n",
            "Epoch:  43 Batch:  5 Current Loss:  6.617662438657135e-05\n",
            "Epoch:  43 Batch:  10 Current Loss:  1.4344047485792544e-05\n",
            "Epoch:  43 Batch:  15 Current Loss:  6.579394539585337e-05\n",
            "Epoch:  43 Batch:  20 Current Loss:  0.00043118378380313516\n",
            "Epoch:  43 Batch:  25 Current Loss:  0.0002195368433604017\n",
            "Epoch:  43 Batch:  30 Current Loss:  0.0003397679829504341\n",
            "Epoch:  43 Batch:  35 Current Loss:  0.00039366824785247445\n",
            "Epoch:  43 Batch:  40 Current Loss:  2.3413304006680846e-06\n",
            "Epoch:  43 Batch:  45 Current Loss:  4.288148829800775e-06\n",
            "Epoch:  43 Batch:  50 Current Loss:  2.865592614398338e-05\n",
            "================================================================================\n",
            "Epoch 43 completed\n",
            "Average train loss is 0.00015255218262994348: \n",
            "Average validation loss is 0.06859749105448525\n",
            "================================================================================\n",
            "Epoch:  44 Batch:  0 Current Loss:  0.00013982980453874916\n",
            "Epoch:  44 Batch:  5 Current Loss:  1.1302042366878595e-05\n",
            "Epoch:  44 Batch:  10 Current Loss:  0.00010083032975671813\n",
            "Epoch:  44 Batch:  15 Current Loss:  0.00020713190315291286\n",
            "Epoch:  44 Batch:  20 Current Loss:  3.959564492106438e-05\n",
            "Epoch:  44 Batch:  25 Current Loss:  0.000142632081406191\n",
            "Epoch:  44 Batch:  30 Current Loss:  8.280873589683324e-05\n",
            "Epoch:  44 Batch:  35 Current Loss:  0.0004307567432988435\n",
            "Epoch:  44 Batch:  40 Current Loss:  6.302865949692205e-05\n",
            "Epoch:  44 Batch:  45 Current Loss:  0.0006756223156116903\n",
            "Epoch:  44 Batch:  50 Current Loss:  0.0009617976611480117\n",
            "================================================================================\n",
            "Epoch 44 completed\n",
            "Average train loss is 0.00020293272203822163: \n",
            "Average validation loss is 0.06899611931294203\n",
            "================================================================================\n",
            "Epoch:  45 Batch:  0 Current Loss:  6.467694674938684e-06\n",
            "Epoch:  45 Batch:  5 Current Loss:  0.00027476155082695186\n",
            "Epoch:  45 Batch:  10 Current Loss:  0.00035658644628711045\n",
            "Epoch:  45 Batch:  15 Current Loss:  0.00020255094568710774\n",
            "Epoch:  45 Batch:  20 Current Loss:  0.00042658820166252553\n",
            "Epoch:  45 Batch:  25 Current Loss:  2.9462000838975655e-06\n",
            "Epoch:  45 Batch:  30 Current Loss:  0.00027603580383583903\n",
            "Epoch:  45 Batch:  35 Current Loss:  2.1160104779482936e-08\n",
            "Epoch:  45 Batch:  40 Current Loss:  0.0003992795245721936\n",
            "Epoch:  45 Batch:  45 Current Loss:  0.00020939545356668532\n",
            "Epoch:  45 Batch:  50 Current Loss:  1.5756382708786987e-05\n",
            "================================================================================\n",
            "Epoch 45 completed\n",
            "Average train loss is 0.00021545333868541953: \n",
            "Average validation loss is 0.06753842858597636\n",
            "================================================================================\n",
            "Epoch:  46 Batch:  0 Current Loss:  0.00035331639810465276\n",
            "Epoch:  46 Batch:  5 Current Loss:  0.00012539952876977623\n",
            "Epoch:  46 Batch:  10 Current Loss:  5.3921750804875046e-05\n",
            "Epoch:  46 Batch:  15 Current Loss:  6.608392141060904e-05\n",
            "Epoch:  46 Batch:  20 Current Loss:  0.00014292185369413346\n",
            "Epoch:  46 Batch:  25 Current Loss:  2.722026692936197e-06\n",
            "Epoch:  46 Batch:  30 Current Loss:  7.974964682944119e-05\n",
            "Epoch:  46 Batch:  35 Current Loss:  7.520072540501133e-05\n",
            "Epoch:  46 Batch:  40 Current Loss:  0.0001155508725787513\n",
            "Epoch:  46 Batch:  45 Current Loss:  3.543052571330918e-06\n",
            "Epoch:  46 Batch:  50 Current Loss:  1.0881421076192055e-05\n",
            "================================================================================\n",
            "Epoch 46 completed\n",
            "Average train loss is 0.00011637155049835603: \n",
            "Average validation loss is 0.06514399436612923\n",
            "================================================================================\n",
            "Epoch:  47 Batch:  0 Current Loss:  4.273966169421328e-06\n",
            "Epoch:  47 Batch:  5 Current Loss:  4.189303581370041e-06\n",
            "Epoch:  47 Batch:  10 Current Loss:  1.4568119865998597e-07\n",
            "Epoch:  47 Batch:  15 Current Loss:  7.606457074871287e-05\n",
            "Epoch:  47 Batch:  20 Current Loss:  8.029564196476713e-05\n",
            "Epoch:  47 Batch:  25 Current Loss:  6.080387038309709e-08\n",
            "Epoch:  47 Batch:  30 Current Loss:  5.409359800978564e-06\n",
            "Epoch:  47 Batch:  35 Current Loss:  0.0002955980016849935\n",
            "Epoch:  47 Batch:  40 Current Loss:  1.342086903832751e-07\n",
            "Epoch:  47 Batch:  45 Current Loss:  0.00011926371371373534\n",
            "Epoch:  47 Batch:  50 Current Loss:  1.848947249527555e-05\n",
            "================================================================================\n",
            "Epoch 47 completed\n",
            "Average train loss is 6.933708124060664e-05: \n",
            "Average validation loss is 0.07060913586368163\n",
            "================================================================================\n",
            "Epoch:  48 Batch:  0 Current Loss:  2.3022472817046946e-07\n",
            "Epoch:  48 Batch:  5 Current Loss:  2.0588302504620515e-05\n",
            "Epoch:  48 Batch:  10 Current Loss:  1.6920843336265534e-05\n",
            "Epoch:  48 Batch:  15 Current Loss:  0.00010424881475046277\n",
            "Epoch:  48 Batch:  20 Current Loss:  2.2672185878036544e-05\n",
            "Epoch:  48 Batch:  25 Current Loss:  2.7221296477364376e-05\n",
            "Epoch:  48 Batch:  30 Current Loss:  7.693688530707732e-05\n",
            "Epoch:  48 Batch:  35 Current Loss:  2.544906055845786e-05\n",
            "Epoch:  48 Batch:  40 Current Loss:  9.24689375096932e-05\n",
            "Epoch:  48 Batch:  45 Current Loss:  5.98608721702476e-07\n",
            "Epoch:  48 Batch:  50 Current Loss:  1.234562569152331e-05\n",
            "================================================================================\n",
            "Epoch 48 completed\n",
            "Average train loss is 0.00010886381533788872: \n",
            "Average validation loss is 0.07018148805946112\n",
            "================================================================================\n",
            "Epoch:  49 Batch:  0 Current Loss:  5.42746647624881e-09\n",
            "Epoch:  49 Batch:  5 Current Loss:  0.00021522943279705942\n",
            "Epoch:  49 Batch:  10 Current Loss:  5.327070175553672e-05\n",
            "Epoch:  49 Batch:  15 Current Loss:  2.582562592579052e-05\n",
            "Epoch:  49 Batch:  20 Current Loss:  2.3137012249208055e-05\n",
            "Epoch:  49 Batch:  25 Current Loss:  5.3409134125104174e-05\n",
            "Epoch:  49 Batch:  30 Current Loss:  9.728481927595567e-06\n",
            "Epoch:  49 Batch:  35 Current Loss:  1.3016961929679383e-05\n",
            "Epoch:  49 Batch:  40 Current Loss:  3.542560079949908e-05\n",
            "Epoch:  49 Batch:  45 Current Loss:  2.8617448606382823e-06\n",
            "Epoch:  49 Batch:  50 Current Loss:  0.00015322465333156288\n",
            "================================================================================\n",
            "Epoch 49 completed\n",
            "Average train loss is 9.746526299718557e-05: \n",
            "Average validation loss is 0.06864950588593881\n",
            "================================================================================\n",
            "Epoch:  50 Batch:  0 Current Loss:  3.0773028356634313e-06\n",
            "Epoch:  50 Batch:  5 Current Loss:  0.00013457941531669348\n",
            "Epoch:  50 Batch:  10 Current Loss:  0.00014143079170025885\n",
            "Epoch:  50 Batch:  15 Current Loss:  4.4322252506390214e-05\n",
            "Epoch:  50 Batch:  20 Current Loss:  0.0001523662213003263\n",
            "Epoch:  50 Batch:  25 Current Loss:  9.080344170797616e-05\n",
            "Epoch:  50 Batch:  30 Current Loss:  1.0435053809487727e-05\n",
            "Epoch:  50 Batch:  35 Current Loss:  7.4065806074941065e-06\n",
            "Epoch:  50 Batch:  40 Current Loss:  2.372671815464855e-06\n",
            "Epoch:  50 Batch:  45 Current Loss:  0.0001325785560766235\n",
            "Epoch:  50 Batch:  50 Current Loss:  4.3698975787265226e-05\n",
            "================================================================================\n",
            "Epoch 50 completed\n",
            "Average train loss is 7.828134776379268e-05: \n",
            "Average validation loss is 0.06947588540303211\n",
            "================================================================================\n",
            "Epoch:  51 Batch:  0 Current Loss:  1.1730455753422575e-06\n",
            "Epoch:  51 Batch:  5 Current Loss:  2.6246470952173695e-05\n",
            "Epoch:  51 Batch:  10 Current Loss:  7.408608325931709e-06\n",
            "Epoch:  51 Batch:  15 Current Loss:  6.126182597654406e-06\n",
            "Epoch:  51 Batch:  20 Current Loss:  2.5860927053145133e-06\n",
            "Epoch:  51 Batch:  25 Current Loss:  0.00012178296310594305\n",
            "Epoch:  51 Batch:  30 Current Loss:  2.1355009494072874e-07\n",
            "Epoch:  51 Batch:  35 Current Loss:  0.00021436165843624622\n",
            "Epoch:  51 Batch:  40 Current Loss:  0.00026452323072589934\n",
            "Epoch:  51 Batch:  45 Current Loss:  0.00020757886522915214\n",
            "Epoch:  51 Batch:  50 Current Loss:  0.0005491459742188454\n",
            "================================================================================\n",
            "Epoch 51 completed\n",
            "Average train loss is 7.433115666488632e-05: \n",
            "Average validation loss is 0.06407494361822803\n",
            "================================================================================\n",
            "Epoch:  52 Batch:  0 Current Loss:  9.116461114899721e-06\n",
            "Epoch:  52 Batch:  5 Current Loss:  0.00012093722762074322\n",
            "Epoch:  52 Batch:  10 Current Loss:  0.00010916711471509188\n",
            "Epoch:  52 Batch:  15 Current Loss:  6.33326317256433e-06\n",
            "Epoch:  52 Batch:  20 Current Loss:  8.154956049111206e-06\n",
            "Epoch:  52 Batch:  25 Current Loss:  0.00011753012950066477\n",
            "Epoch:  52 Batch:  30 Current Loss:  0.00013436342123895884\n",
            "Epoch:  52 Batch:  35 Current Loss:  2.8146002932771808e-06\n",
            "Epoch:  52 Batch:  40 Current Loss:  1.2319146662775893e-05\n",
            "Epoch:  52 Batch:  45 Current Loss:  6.958346148167038e-07\n",
            "Epoch:  52 Batch:  50 Current Loss:  0.00016498661716468632\n",
            "================================================================================\n",
            "Epoch 52 completed\n",
            "Average train loss is 6.303754677798697e-05: \n",
            "Average validation loss is 0.0713242533383891\n",
            "================================================================================\n",
            "Epoch:  53 Batch:  0 Current Loss:  4.3581541831372306e-05\n",
            "Epoch:  53 Batch:  5 Current Loss:  1.8355673091718927e-05\n",
            "Epoch:  53 Batch:  10 Current Loss:  7.84266012487933e-05\n",
            "Epoch:  53 Batch:  15 Current Loss:  2.2992024241830222e-05\n",
            "Epoch:  53 Batch:  20 Current Loss:  5.338408300303854e-06\n",
            "Epoch:  53 Batch:  25 Current Loss:  1.388178498018533e-05\n",
            "Epoch:  53 Batch:  30 Current Loss:  1.534243165224325e-05\n",
            "Epoch:  53 Batch:  35 Current Loss:  6.224059325177222e-05\n",
            "Epoch:  53 Batch:  40 Current Loss:  2.0816312826354988e-05\n",
            "Epoch:  53 Batch:  45 Current Loss:  6.17642144788988e-05\n",
            "Epoch:  53 Batch:  50 Current Loss:  9.751734978635795e-06\n",
            "================================================================================\n",
            "Epoch 53 completed\n",
            "Average train loss is 5.2041821391658214e-05: \n",
            "Average validation loss is 0.0654466775401185\n",
            "================================================================================\n",
            "Epoch:  54 Batch:  0 Current Loss:  5.781063919130247e-06\n",
            "Epoch:  54 Batch:  5 Current Loss:  1.1713701496773865e-05\n",
            "Epoch:  54 Batch:  10 Current Loss:  2.7085410692961887e-05\n",
            "Epoch:  54 Batch:  15 Current Loss:  2.4782868422335014e-05\n",
            "Epoch:  54 Batch:  20 Current Loss:  1.9393013417356997e-08\n",
            "Epoch:  54 Batch:  25 Current Loss:  2.147941449948121e-05\n",
            "Epoch:  54 Batch:  30 Current Loss:  5.3733492677565664e-05\n",
            "Epoch:  54 Batch:  35 Current Loss:  0.0003395890526007861\n",
            "Epoch:  54 Batch:  40 Current Loss:  5.9051410062238574e-05\n",
            "Epoch:  54 Batch:  45 Current Loss:  6.474820679613913e-09\n",
            "Epoch:  54 Batch:  50 Current Loss:  5.696292282664217e-05\n",
            "================================================================================\n",
            "Epoch 54 completed\n",
            "Average train loss is 4.826776429457989e-05: \n",
            "Average validation loss is 0.06510750219846766\n",
            "================================================================================\n",
            "Epoch:  55 Batch:  0 Current Loss:  4.375090429675765e-05\n",
            "Epoch:  55 Batch:  5 Current Loss:  6.85802660882473e-07\n",
            "Epoch:  55 Batch:  10 Current Loss:  2.8103520435251994e-06\n",
            "Epoch:  55 Batch:  15 Current Loss:  9.26454049476888e-06\n",
            "Epoch:  55 Batch:  20 Current Loss:  0.00012039019202347845\n",
            "Epoch:  55 Batch:  25 Current Loss:  3.1538471375824884e-05\n",
            "Epoch:  55 Batch:  30 Current Loss:  5.532944487640634e-05\n",
            "Epoch:  55 Batch:  35 Current Loss:  3.9630722312722355e-05\n",
            "Epoch:  55 Batch:  40 Current Loss:  1.7434418623452075e-05\n",
            "Epoch:  55 Batch:  45 Current Loss:  5.535582658922067e-09\n",
            "Epoch:  55 Batch:  50 Current Loss:  3.180943167535588e-05\n",
            "================================================================================\n",
            "Epoch 55 completed\n",
            "Average train loss is 4.4794795828879005e-05: \n",
            "Average validation loss is 0.07308588727998237\n",
            "================================================================================\n",
            "Epoch:  56 Batch:  0 Current Loss:  4.786818317370489e-05\n",
            "Epoch:  56 Batch:  5 Current Loss:  3.7178224374656565e-06\n",
            "Epoch:  56 Batch:  10 Current Loss:  4.2552615923341364e-05\n",
            "Epoch:  56 Batch:  15 Current Loss:  9.174701744996128e-07\n",
            "Epoch:  56 Batch:  20 Current Loss:  1.604847057024017e-05\n",
            "Epoch:  56 Batch:  25 Current Loss:  4.8327648983104154e-05\n",
            "Epoch:  56 Batch:  30 Current Loss:  0.00013212701014708728\n",
            "Epoch:  56 Batch:  35 Current Loss:  6.092592684581177e-06\n",
            "Epoch:  56 Batch:  40 Current Loss:  8.578953156757052e-07\n",
            "Epoch:  56 Batch:  45 Current Loss:  2.610424189697369e-06\n",
            "Epoch:  56 Batch:  50 Current Loss:  0.00010347796342102811\n",
            "================================================================================\n",
            "Epoch 56 completed\n",
            "Average train loss is 3.228253860247772e-05: \n",
            "Average validation loss is 0.07247074103603761\n",
            "================================================================================\n",
            "Epoch:  57 Batch:  0 Current Loss:  6.882630259497091e-05\n",
            "Epoch:  57 Batch:  5 Current Loss:  1.1113953632957418e-06\n",
            "Epoch:  57 Batch:  10 Current Loss:  1.0609643652514933e-07\n",
            "Epoch:  57 Batch:  15 Current Loss:  8.874208288034424e-05\n",
            "Epoch:  57 Batch:  20 Current Loss:  1.5438712580362335e-05\n",
            "Epoch:  57 Batch:  25 Current Loss:  1.777170655259397e-05\n",
            "Epoch:  57 Batch:  30 Current Loss:  1.0411359880890814e-06\n",
            "Epoch:  57 Batch:  35 Current Loss:  2.8310778361628763e-05\n",
            "Epoch:  57 Batch:  40 Current Loss:  4.8491667257621884e-05\n",
            "Epoch:  57 Batch:  45 Current Loss:  2.6759059892356163e-06\n",
            "Epoch:  57 Batch:  50 Current Loss:  2.8113068765378557e-05\n",
            "================================================================================\n",
            "Epoch 57 completed\n",
            "Average train loss is 2.576680731597874e-05: \n",
            "Average validation loss is 0.07236160306880872\n",
            "================================================================================\n",
            "Epoch:  58 Batch:  0 Current Loss:  2.3118953322409652e-05\n",
            "Epoch:  58 Batch:  5 Current Loss:  2.7015816158382222e-05\n",
            "Epoch:  58 Batch:  10 Current Loss:  2.3119812908589665e-07\n",
            "Epoch:  58 Batch:  15 Current Loss:  7.204840949270874e-05\n",
            "Epoch:  58 Batch:  20 Current Loss:  1.3125303439665004e-06\n",
            "Epoch:  58 Batch:  25 Current Loss:  1.5061295925988816e-06\n",
            "Epoch:  58 Batch:  30 Current Loss:  2.006273689403315e-06\n",
            "Epoch:  58 Batch:  35 Current Loss:  1.1332749636494555e-06\n",
            "Epoch:  58 Batch:  40 Current Loss:  1.901166069728788e-05\n",
            "Epoch:  58 Batch:  45 Current Loss:  9.697181667434052e-06\n",
            "Epoch:  58 Batch:  50 Current Loss:  5.59587078896584e-06\n",
            "================================================================================\n",
            "Epoch 58 completed\n",
            "Average train loss is 3.979327601498174e-05: \n",
            "Average validation loss is 0.06799683440476656\n",
            "================================================================================\n",
            "Epoch:  59 Batch:  0 Current Loss:  7.414715946651995e-05\n",
            "Epoch:  59 Batch:  5 Current Loss:  1.6355954812752316e-07\n",
            "Epoch:  59 Batch:  10 Current Loss:  2.0969767433598463e-07\n",
            "Epoch:  59 Batch:  15 Current Loss:  2.2641119358013384e-05\n",
            "Epoch:  59 Batch:  20 Current Loss:  9.732479156809859e-06\n",
            "Epoch:  59 Batch:  25 Current Loss:  1.4863016076560598e-05\n",
            "Epoch:  59 Batch:  30 Current Loss:  4.718504840184323e-07\n",
            "Epoch:  59 Batch:  35 Current Loss:  4.394579264044296e-06\n",
            "Epoch:  59 Batch:  40 Current Loss:  5.276119554764591e-05\n",
            "Epoch:  59 Batch:  45 Current Loss:  7.182193076005206e-05\n",
            "Epoch:  59 Batch:  50 Current Loss:  2.500025038898457e-05\n",
            "================================================================================\n",
            "Epoch 59 completed\n",
            "Average train loss is 3.114354496747983e-05: \n",
            "Average validation loss is 0.06975874894609053\n",
            "================================================================================\n",
            "Epoch:  60 Batch:  0 Current Loss:  1.7637592463870533e-05\n",
            "Epoch:  60 Batch:  5 Current Loss:  1.7234649931197055e-05\n",
            "Epoch:  60 Batch:  10 Current Loss:  2.868752198992297e-05\n",
            "Epoch:  60 Batch:  15 Current Loss:  2.395535193500109e-05\n",
            "Epoch:  60 Batch:  20 Current Loss:  7.867409294703975e-05\n",
            "Epoch:  60 Batch:  25 Current Loss:  3.8661130474793026e-07\n",
            "Epoch:  60 Batch:  30 Current Loss:  6.280796696955804e-06\n",
            "Epoch:  60 Batch:  35 Current Loss:  3.424676469876431e-05\n",
            "Epoch:  60 Batch:  40 Current Loss:  3.001136428792961e-05\n",
            "Epoch:  60 Batch:  45 Current Loss:  2.5359202027175343e-06\n",
            "Epoch:  60 Batch:  50 Current Loss:  7.019785698503256e-05\n",
            "================================================================================\n",
            "Epoch 60 completed\n",
            "Average train loss is 4.0051062373053655e-05: \n",
            "Average validation loss is 0.07380478999887903\n",
            "================================================================================\n",
            "Epoch:  61 Batch:  0 Current Loss:  6.637605838477612e-05\n",
            "Epoch:  61 Batch:  5 Current Loss:  3.590150663512759e-05\n",
            "Epoch:  61 Batch:  10 Current Loss:  3.900991578120738e-05\n",
            "Epoch:  61 Batch:  15 Current Loss:  3.7832716770935804e-05\n",
            "Epoch:  61 Batch:  20 Current Loss:  4.5024003725302464e-07\n",
            "Epoch:  61 Batch:  25 Current Loss:  9.484718361818523e-07\n",
            "Epoch:  61 Batch:  30 Current Loss:  1.1797592378570698e-05\n",
            "Epoch:  61 Batch:  35 Current Loss:  8.219783921958879e-05\n",
            "Epoch:  61 Batch:  40 Current Loss:  0.00015510033699683845\n",
            "Epoch:  61 Batch:  45 Current Loss:  3.0714865715708584e-05\n",
            "Epoch:  61 Batch:  50 Current Loss:  7.851556915738911e-07\n",
            "================================================================================\n",
            "Epoch 61 completed\n",
            "Average train loss is 3.2278793930349785e-05: \n",
            "Average validation loss is 0.07539775067319472\n",
            "================================================================================\n",
            "Epoch:  62 Batch:  0 Current Loss:  2.509318392185378e-06\n",
            "Epoch:  62 Batch:  5 Current Loss:  4.483582597458735e-05\n",
            "Epoch:  62 Batch:  10 Current Loss:  6.734886846970767e-05\n",
            "Epoch:  62 Batch:  15 Current Loss:  1.9966184936492937e-06\n",
            "Epoch:  62 Batch:  20 Current Loss:  1.5926001651678234e-05\n",
            "Epoch:  62 Batch:  25 Current Loss:  4.8547921323915944e-05\n",
            "Epoch:  62 Batch:  30 Current Loss:  0.00013719152775593102\n",
            "Epoch:  62 Batch:  35 Current Loss:  3.181634610882611e-06\n",
            "Epoch:  62 Batch:  40 Current Loss:  6.0899841628270224e-05\n",
            "Epoch:  62 Batch:  45 Current Loss:  2.753488672624371e-07\n",
            "Epoch:  62 Batch:  50 Current Loss:  2.1815381501255615e-08\n",
            "================================================================================\n",
            "Epoch 62 completed\n",
            "Average train loss is 4.579302076384701e-05: \n",
            "Average validation loss is 0.06751344283111393\n",
            "================================================================================\n",
            "Epoch:  63 Batch:  0 Current Loss:  4.938061465509236e-05\n",
            "Epoch:  63 Batch:  5 Current Loss:  4.440304564923281e-06\n",
            "Epoch:  63 Batch:  10 Current Loss:  0.00010930323333013803\n",
            "Epoch:  63 Batch:  15 Current Loss:  0.0001411332341376692\n",
            "Epoch:  63 Batch:  20 Current Loss:  9.581907534084166e-07\n",
            "Epoch:  63 Batch:  25 Current Loss:  8.114120282698423e-05\n",
            "Epoch:  63 Batch:  30 Current Loss:  1.920309296110645e-05\n",
            "Epoch:  63 Batch:  35 Current Loss:  5.072217754786834e-05\n",
            "Epoch:  63 Batch:  40 Current Loss:  9.376907655678224e-06\n",
            "Epoch:  63 Batch:  45 Current Loss:  5.6969220167957246e-05\n",
            "Epoch:  63 Batch:  50 Current Loss:  3.3905434975167736e-05\n",
            "================================================================================\n",
            "Epoch 63 completed\n",
            "Average train loss is 2.926418945016816e-05: \n",
            "Average validation loss is 0.06814145715907216\n",
            "================================================================================\n",
            "Epoch:  64 Batch:  0 Current Loss:  8.142110345943365e-06\n",
            "Epoch:  64 Batch:  5 Current Loss:  1.7074199831768055e-08\n",
            "Epoch:  64 Batch:  10 Current Loss:  1.353717834717827e-05\n",
            "Epoch:  64 Batch:  15 Current Loss:  5.7252000260632485e-06\n",
            "Epoch:  64 Batch:  20 Current Loss:  1.5040455991766066e-06\n",
            "Epoch:  64 Batch:  25 Current Loss:  4.105981861357577e-05\n",
            "Epoch:  64 Batch:  30 Current Loss:  2.0521014448604546e-05\n",
            "Epoch:  64 Batch:  35 Current Loss:  5.017743205826264e-06\n",
            "Epoch:  64 Batch:  40 Current Loss:  2.8270637812966015e-06\n",
            "Epoch:  64 Batch:  45 Current Loss:  2.1198571630520746e-05\n",
            "Epoch:  64 Batch:  50 Current Loss:  2.1667392502422445e-05\n",
            "================================================================================\n",
            "Epoch 64 completed\n",
            "Average train loss is 2.0176671565388722e-05: \n",
            "Average validation loss is 0.06495419571486612\n",
            "================================================================================\n",
            "Epoch:  65 Batch:  0 Current Loss:  1.3397515431279317e-05\n",
            "Epoch:  65 Batch:  5 Current Loss:  2.7252729069005e-06\n",
            "Epoch:  65 Batch:  10 Current Loss:  1.1653599358396605e-05\n",
            "Epoch:  65 Batch:  15 Current Loss:  1.3229726391728036e-05\n",
            "Epoch:  65 Batch:  20 Current Loss:  2.759927519946359e-05\n",
            "Epoch:  65 Batch:  25 Current Loss:  1.116459952754667e-05\n",
            "Epoch:  65 Batch:  30 Current Loss:  2.9567306114586245e-07\n",
            "Epoch:  65 Batch:  35 Current Loss:  3.611693728089449e-06\n",
            "Epoch:  65 Batch:  40 Current Loss:  2.052033960353583e-05\n",
            "Epoch:  65 Batch:  45 Current Loss:  1.2510574379120953e-05\n",
            "Epoch:  65 Batch:  50 Current Loss:  2.7534872515388997e-06\n",
            "================================================================================\n",
            "Epoch 65 completed\n",
            "Average train loss is 1.5540676444381063e-05: \n",
            "Average validation loss is 0.06818798991541068\n",
            "================================================================================\n",
            "Epoch:  66 Batch:  0 Current Loss:  0.00012697867350652814\n",
            "Epoch:  66 Batch:  5 Current Loss:  1.2603931232035848e-09\n",
            "Epoch:  66 Batch:  10 Current Loss:  2.308159537278698e-06\n",
            "Epoch:  66 Batch:  15 Current Loss:  8.354613237315789e-05\n",
            "Epoch:  66 Batch:  20 Current Loss:  7.835426004021429e-06\n",
            "Epoch:  66 Batch:  25 Current Loss:  1.3049750123172998e-05\n",
            "Epoch:  66 Batch:  30 Current Loss:  1.0222424862149637e-05\n",
            "Epoch:  66 Batch:  35 Current Loss:  5.318068724591285e-05\n",
            "Epoch:  66 Batch:  40 Current Loss:  3.4488828077883227e-06\n",
            "Epoch:  66 Batch:  45 Current Loss:  1.339788695986499e-06\n",
            "Epoch:  66 Batch:  50 Current Loss:  6.142448546597734e-05\n",
            "================================================================================\n",
            "Epoch 66 completed\n",
            "Average train loss is 2.5383263435980913e-05: \n",
            "Average validation loss is 0.07063296778748433\n",
            "================================================================================\n",
            "Epoch:  67 Batch:  0 Current Loss:  4.755498139275005e-06\n",
            "Epoch:  67 Batch:  5 Current Loss:  3.95707075995233e-07\n",
            "Epoch:  67 Batch:  10 Current Loss:  1.4075440049055032e-05\n",
            "Epoch:  67 Batch:  15 Current Loss:  1.2925271221320145e-05\n",
            "Epoch:  67 Batch:  20 Current Loss:  9.048111593301655e-09\n",
            "Epoch:  67 Batch:  25 Current Loss:  2.4212615244323388e-05\n",
            "Epoch:  67 Batch:  30 Current Loss:  2.2197331418283284e-05\n",
            "Epoch:  67 Batch:  35 Current Loss:  3.21138884373795e-07\n",
            "Epoch:  67 Batch:  40 Current Loss:  1.798599259927869e-05\n",
            "Epoch:  67 Batch:  45 Current Loss:  7.572143204015447e-06\n",
            "Epoch:  67 Batch:  50 Current Loss:  9.990119906433392e-06\n",
            "================================================================================\n",
            "Epoch 67 completed\n",
            "Average train loss is 1.7042045199785386e-05: \n",
            "Average validation loss is 0.0693437341445436\n",
            "================================================================================\n",
            "Epoch:  68 Batch:  0 Current Loss:  4.507081030169502e-05\n",
            "Epoch:  68 Batch:  5 Current Loss:  1.4313221981865354e-05\n",
            "Epoch:  68 Batch:  10 Current Loss:  7.435808129230281e-06\n",
            "Epoch:  68 Batch:  15 Current Loss:  4.1785747271205764e-06\n",
            "Epoch:  68 Batch:  20 Current Loss:  2.8431213650037535e-06\n",
            "Epoch:  68 Batch:  25 Current Loss:  3.284625563537702e-05\n",
            "Epoch:  68 Batch:  30 Current Loss:  3.241474405513145e-05\n",
            "Epoch:  68 Batch:  35 Current Loss:  2.3705147214059252e-06\n",
            "Epoch:  68 Batch:  40 Current Loss:  8.189545951609034e-06\n",
            "Epoch:  68 Batch:  45 Current Loss:  1.5953008869473706e-06\n",
            "Epoch:  68 Batch:  50 Current Loss:  3.876547998515889e-05\n",
            "================================================================================\n",
            "Epoch 68 completed\n",
            "Average train loss is 1.399405670949722e-05: \n",
            "Average validation loss is 0.07134796679019928\n",
            "================================================================================\n",
            "Epoch:  69 Batch:  0 Current Loss:  1.2540318493847735e-05\n",
            "Epoch:  69 Batch:  5 Current Loss:  1.3505205970432144e-05\n",
            "Epoch:  69 Batch:  10 Current Loss:  5.074916487046721e-08\n",
            "Epoch:  69 Batch:  15 Current Loss:  1.3125544683134649e-05\n",
            "Epoch:  69 Batch:  20 Current Loss:  8.801939657132607e-06\n",
            "Epoch:  69 Batch:  25 Current Loss:  9.266173947253264e-06\n",
            "Epoch:  69 Batch:  30 Current Loss:  1.4399747669813223e-05\n",
            "Epoch:  69 Batch:  35 Current Loss:  8.74844033660338e-07\n",
            "Epoch:  69 Batch:  40 Current Loss:  1.4101728083915077e-05\n",
            "Epoch:  69 Batch:  45 Current Loss:  2.439376839902252e-05\n",
            "Epoch:  69 Batch:  50 Current Loss:  4.16251538126744e-07\n",
            "================================================================================\n",
            "Epoch 69 completed\n",
            "Average train loss is 1.1950177437227193e-05: \n",
            "Average validation loss is 0.07106643565930426\n",
            "================================================================================\n",
            "Epoch:  70 Batch:  0 Current Loss:  1.0711279173847288e-05\n",
            "Epoch:  70 Batch:  5 Current Loss:  1.715845792205073e-05\n",
            "Epoch:  70 Batch:  10 Current Loss:  2.5438461307203397e-05\n",
            "Epoch:  70 Batch:  15 Current Loss:  4.792149775312282e-05\n",
            "Epoch:  70 Batch:  20 Current Loss:  2.7726946427719668e-05\n",
            "Epoch:  70 Batch:  25 Current Loss:  4.029576672337498e-09\n",
            "Epoch:  70 Batch:  30 Current Loss:  8.21545472717844e-05\n",
            "Epoch:  70 Batch:  35 Current Loss:  2.4765067792031914e-05\n",
            "Epoch:  70 Batch:  40 Current Loss:  3.333966378704645e-05\n",
            "Epoch:  70 Batch:  45 Current Loss:  6.361494797602063e-06\n",
            "Epoch:  70 Batch:  50 Current Loss:  2.0813849914702587e-06\n",
            "================================================================================\n",
            "Epoch 70 completed\n",
            "Average train loss is 1.68820663003414e-05: \n",
            "Average validation loss is 0.07467668456956744\n",
            "================================================================================\n",
            "Epoch:  71 Batch:  0 Current Loss:  1.9146036720485426e-06\n",
            "Epoch:  71 Batch:  5 Current Loss:  1.8026803445536643e-06\n",
            "Epoch:  71 Batch:  10 Current Loss:  1.7969061445910484e-05\n",
            "Epoch:  71 Batch:  15 Current Loss:  2.24176183110103e-05\n",
            "Epoch:  71 Batch:  20 Current Loss:  1.8711962468387355e-07\n",
            "Epoch:  71 Batch:  25 Current Loss:  1.9012660459338804e-06\n",
            "Epoch:  71 Batch:  30 Current Loss:  1.2483603484270134e-07\n",
            "Epoch:  71 Batch:  35 Current Loss:  5.002606485504657e-05\n",
            "Epoch:  71 Batch:  40 Current Loss:  1.4478906450676732e-05\n",
            "Epoch:  71 Batch:  45 Current Loss:  3.0733308449271135e-06\n",
            "Epoch:  71 Batch:  50 Current Loss:  2.1843374270247295e-05\n",
            "================================================================================\n",
            "Epoch 71 completed\n",
            "Average train loss is 1.4668889491155677e-05: \n",
            "Average validation loss is 0.0703409081324935\n",
            "================================================================================\n",
            "Epoch:  72 Batch:  0 Current Loss:  6.394611318683019e-06\n",
            "Epoch:  72 Batch:  5 Current Loss:  3.0340423506913794e-09\n",
            "Epoch:  72 Batch:  10 Current Loss:  7.719194400124252e-06\n",
            "Epoch:  72 Batch:  15 Current Loss:  6.075886460621405e-09\n",
            "Epoch:  72 Batch:  20 Current Loss:  1.1966045931899316e-08\n",
            "Epoch:  72 Batch:  25 Current Loss:  4.5680292259930866e-07\n",
            "Epoch:  72 Batch:  30 Current Loss:  1.912038351292722e-05\n",
            "Epoch:  72 Batch:  35 Current Loss:  1.9142325982102193e-06\n",
            "Epoch:  72 Batch:  40 Current Loss:  1.038658865581965e-05\n",
            "Epoch:  72 Batch:  45 Current Loss:  4.964389881934039e-05\n",
            "Epoch:  72 Batch:  50 Current Loss:  1.1228022231080104e-05\n",
            "================================================================================\n",
            "Epoch 72 completed\n",
            "Average train loss is 1.0249859411277821e-05: \n",
            "Average validation loss is 0.06411791282395522\n",
            "================================================================================\n",
            "Epoch:  73 Batch:  0 Current Loss:  1.2322376505835564e-06\n",
            "Epoch:  73 Batch:  5 Current Loss:  1.5379166143247858e-05\n",
            "Epoch:  73 Batch:  10 Current Loss:  3.906987728896638e-07\n",
            "Epoch:  73 Batch:  15 Current Loss:  1.7953620044863783e-05\n",
            "Epoch:  73 Batch:  20 Current Loss:  2.4548713554395363e-05\n",
            "Epoch:  73 Batch:  25 Current Loss:  1.6704616427887231e-06\n",
            "Epoch:  73 Batch:  30 Current Loss:  1.8445753084961325e-06\n",
            "Epoch:  73 Batch:  35 Current Loss:  1.824927494453732e-05\n",
            "Epoch:  73 Batch:  40 Current Loss:  9.170486009679735e-06\n",
            "Epoch:  73 Batch:  45 Current Loss:  1.3289956768858247e-05\n",
            "Epoch:  73 Batch:  50 Current Loss:  3.8342968764482066e-05\n",
            "================================================================================\n",
            "Epoch 73 completed\n",
            "Average train loss is 1.5223508403148746e-05: \n",
            "Average validation loss is 0.076844345467786\n",
            "================================================================================\n",
            "Epoch:  74 Batch:  0 Current Loss:  2.5171608285745606e-05\n",
            "Epoch:  74 Batch:  5 Current Loss:  4.665445885621011e-06\n",
            "Epoch:  74 Batch:  10 Current Loss:  8.289892321045045e-06\n",
            "Epoch:  74 Batch:  15 Current Loss:  4.765771791426232e-06\n",
            "Epoch:  74 Batch:  20 Current Loss:  6.0861588281113654e-05\n",
            "Epoch:  74 Batch:  25 Current Loss:  1.6372807749576168e-06\n",
            "Epoch:  74 Batch:  30 Current Loss:  6.645268149441108e-05\n",
            "Epoch:  74 Batch:  35 Current Loss:  1.6368010619771667e-05\n",
            "Epoch:  74 Batch:  40 Current Loss:  1.4969830886002455e-07\n",
            "Epoch:  74 Batch:  45 Current Loss:  5.4670588056637826e-09\n",
            "Epoch:  74 Batch:  50 Current Loss:  1.3261940694064833e-05\n",
            "================================================================================\n",
            "Epoch 74 completed\n",
            "Average train loss is 2.2949664297407352e-05: \n",
            "Average validation loss is 0.06633359861249725\n",
            "================================================================================\n",
            "Epoch:  75 Batch:  0 Current Loss:  1.9938610421377234e-05\n",
            "Epoch:  75 Batch:  5 Current Loss:  6.187708095239941e-06\n",
            "Epoch:  75 Batch:  10 Current Loss:  2.2232026822166517e-05\n",
            "Epoch:  75 Batch:  15 Current Loss:  1.7202951312356163e-06\n",
            "Epoch:  75 Batch:  20 Current Loss:  2.452037222155923e-07\n",
            "Epoch:  75 Batch:  25 Current Loss:  6.1241175899340305e-06\n",
            "Epoch:  75 Batch:  30 Current Loss:  1.81391260412056e-05\n",
            "Epoch:  75 Batch:  35 Current Loss:  6.063790351618081e-06\n",
            "Epoch:  75 Batch:  40 Current Loss:  9.252948802895844e-05\n",
            "Epoch:  75 Batch:  45 Current Loss:  3.9658868900005473e-07\n",
            "Epoch:  75 Batch:  50 Current Loss:  1.9123381207464263e-05\n",
            "================================================================================\n",
            "Epoch 75 completed\n",
            "Average train loss is 9.285961355939201e-06: \n",
            "Average validation loss is 0.07213239620129268\n",
            "================================================================================\n",
            "Epoch:  76 Batch:  0 Current Loss:  2.3492718810302904e-06\n",
            "Epoch:  76 Batch:  5 Current Loss:  7.701981303398497e-06\n",
            "Epoch:  76 Batch:  10 Current Loss:  2.2822882783657406e-06\n",
            "Epoch:  76 Batch:  15 Current Loss:  1.6068146578618325e-05\n",
            "Epoch:  76 Batch:  20 Current Loss:  2.2359272406902164e-06\n",
            "Epoch:  76 Batch:  25 Current Loss:  1.8149979950976558e-05\n",
            "Epoch:  76 Batch:  30 Current Loss:  1.2164421150373528e-06\n",
            "Epoch:  76 Batch:  35 Current Loss:  1.3952717381471302e-05\n",
            "Epoch:  76 Batch:  40 Current Loss:  1.3430315448204055e-05\n",
            "Epoch:  76 Batch:  45 Current Loss:  1.236741013599385e-06\n",
            "Epoch:  76 Batch:  50 Current Loss:  5.858242730027996e-06\n",
            "================================================================================\n",
            "Epoch 76 completed\n",
            "Average train loss is 9.070228271909833e-06: \n",
            "Average validation loss is 0.06692594739918907\n",
            "================================================================================\n",
            "Epoch:  77 Batch:  0 Current Loss:  4.5714540419794503e-07\n",
            "Epoch:  77 Batch:  5 Current Loss:  2.091017086058855e-05\n",
            "Epoch:  77 Batch:  10 Current Loss:  2.5568046112312004e-05\n",
            "Epoch:  77 Batch:  15 Current Loss:  7.196003934950568e-06\n",
            "Epoch:  77 Batch:  20 Current Loss:  2.9785667265969096e-08\n",
            "Epoch:  77 Batch:  25 Current Loss:  7.787781214574352e-06\n",
            "Epoch:  77 Batch:  30 Current Loss:  5.6497201512684114e-06\n",
            "Epoch:  77 Batch:  35 Current Loss:  3.737232327694073e-05\n",
            "Epoch:  77 Batch:  40 Current Loss:  1.5695044567110017e-05\n",
            "Epoch:  77 Batch:  45 Current Loss:  2.6071866159327328e-05\n",
            "Epoch:  77 Batch:  50 Current Loss:  3.809433657409045e-09\n",
            "================================================================================\n",
            "Epoch 77 completed\n",
            "Average train loss is 9.543533396336392e-06: \n",
            "Average validation loss is 0.07228769424060981\n",
            "================================================================================\n",
            "Epoch:  78 Batch:  0 Current Loss:  5.578189757216023e-06\n",
            "Epoch:  78 Batch:  5 Current Loss:  3.0677952054247726e-06\n",
            "Epoch:  78 Batch:  10 Current Loss:  3.2844206998561276e-07\n",
            "Epoch:  78 Batch:  15 Current Loss:  3.2498803648195462e-06\n",
            "Epoch:  78 Batch:  20 Current Loss:  4.569976681523258e-06\n",
            "Epoch:  78 Batch:  25 Current Loss:  7.109524176485138e-06\n",
            "Epoch:  78 Batch:  30 Current Loss:  8.046975381148513e-06\n",
            "Epoch:  78 Batch:  35 Current Loss:  2.2144428157844231e-07\n",
            "Epoch:  78 Batch:  40 Current Loss:  1.7990038031712174e-05\n",
            "Epoch:  78 Batch:  45 Current Loss:  1.545481836728868e-06\n",
            "Epoch:  78 Batch:  50 Current Loss:  4.3314770437064e-07\n",
            "================================================================================\n",
            "Epoch 78 completed\n",
            "Average train loss is 7.410220768845942e-06: \n",
            "Average validation loss is 0.0661605113806824\n",
            "================================================================================\n",
            "Epoch:  79 Batch:  0 Current Loss:  1.0648902843968244e-06\n",
            "Epoch:  79 Batch:  5 Current Loss:  6.987773986111279e-07\n",
            "Epoch:  79 Batch:  10 Current Loss:  2.967570026157773e-06\n",
            "Epoch:  79 Batch:  15 Current Loss:  1.897651600302197e-06\n",
            "Epoch:  79 Batch:  20 Current Loss:  1.1408292266423814e-05\n",
            "Epoch:  79 Batch:  25 Current Loss:  1.3813469479373452e-07\n",
            "Epoch:  79 Batch:  30 Current Loss:  9.423050869372673e-06\n",
            "Epoch:  79 Batch:  35 Current Loss:  8.81966627730435e-07\n",
            "Epoch:  79 Batch:  40 Current Loss:  1.70203077232145e-06\n",
            "Epoch:  79 Batch:  45 Current Loss:  1.696021899988409e-05\n",
            "Epoch:  79 Batch:  50 Current Loss:  5.378697096602991e-06\n",
            "================================================================================\n",
            "Epoch 79 completed\n",
            "Average train loss is 7.315386684516931e-06: \n",
            "Average validation loss is 0.0705843170095856\n",
            "================================================================================\n",
            "Epoch:  80 Batch:  0 Current Loss:  1.501945376958247e-07\n",
            "Epoch:  80 Batch:  5 Current Loss:  1.564431659062393e-05\n",
            "Epoch:  80 Batch:  10 Current Loss:  4.040769908897346e-06\n",
            "Epoch:  80 Batch:  15 Current Loss:  3.8112636957521318e-06\n",
            "Epoch:  80 Batch:  20 Current Loss:  2.383195862876164e-10\n",
            "Epoch:  80 Batch:  25 Current Loss:  4.806855577044189e-05\n",
            "Epoch:  80 Batch:  30 Current Loss:  6.995686021582515e-07\n",
            "Epoch:  80 Batch:  35 Current Loss:  2.6674564423956326e-07\n",
            "Epoch:  80 Batch:  40 Current Loss:  8.0866739153862e-06\n",
            "Epoch:  80 Batch:  45 Current Loss:  2.1338221358746523e-06\n",
            "Epoch:  80 Batch:  50 Current Loss:  1.4102399291004986e-05\n",
            "================================================================================\n",
            "Epoch 80 completed\n",
            "Average train loss is 6.955038476936109e-06: \n",
            "Average validation loss is 0.0687257251702249\n",
            "================================================================================\n",
            "Epoch:  81 Batch:  0 Current Loss:  2.549892315073521e-06\n",
            "Epoch:  81 Batch:  5 Current Loss:  1.9240462279412895e-05\n",
            "Epoch:  81 Batch:  10 Current Loss:  1.038351547322236e-05\n",
            "Epoch:  81 Batch:  15 Current Loss:  3.4663807468859886e-07\n",
            "Epoch:  81 Batch:  20 Current Loss:  2.7907864932785742e-05\n",
            "Epoch:  81 Batch:  25 Current Loss:  3.286894980192301e-06\n",
            "Epoch:  81 Batch:  30 Current Loss:  4.114938201382756e-05\n",
            "Epoch:  81 Batch:  35 Current Loss:  2.3262831746251322e-05\n",
            "Epoch:  81 Batch:  40 Current Loss:  1.9134720787405968e-05\n",
            "Epoch:  81 Batch:  45 Current Loss:  7.123056275304407e-05\n",
            "Epoch:  81 Batch:  50 Current Loss:  1.3067304394098755e-07\n",
            "================================================================================\n",
            "Epoch 81 completed\n",
            "Average train loss is 1.3353675703909628e-05: \n",
            "Average validation loss is 0.06634563479262094\n",
            "================================================================================\n",
            "Epoch:  82 Batch:  0 Current Loss:  5.218790647631977e-06\n",
            "Epoch:  82 Batch:  5 Current Loss:  2.259973115315006e-07\n",
            "Epoch:  82 Batch:  10 Current Loss:  2.6669062208384275e-05\n",
            "Epoch:  82 Batch:  15 Current Loss:  3.544192281879077e-07\n",
            "Epoch:  82 Batch:  20 Current Loss:  2.6413539444547496e-07\n",
            "Epoch:  82 Batch:  25 Current Loss:  4.1371905012965726e-08\n",
            "Epoch:  82 Batch:  30 Current Loss:  1.4280880350270309e-05\n",
            "Epoch:  82 Batch:  35 Current Loss:  5.465092726808507e-06\n",
            "Epoch:  82 Batch:  40 Current Loss:  3.178047336405143e-06\n",
            "Epoch:  82 Batch:  45 Current Loss:  1.0626152402437583e-07\n",
            "Epoch:  82 Batch:  50 Current Loss:  1.7456452042097226e-05\n",
            "================================================================================\n",
            "Epoch 82 completed\n",
            "Average train loss is 6.118838372460599e-06: \n",
            "Average validation loss is 0.07220775017049164\n",
            "================================================================================\n",
            "Epoch:  83 Batch:  0 Current Loss:  4.579240339808166e-05\n",
            "Epoch:  83 Batch:  5 Current Loss:  3.95034203393152e-06\n",
            "Epoch:  83 Batch:  10 Current Loss:  7.899792819898721e-08\n",
            "Epoch:  83 Batch:  15 Current Loss:  9.611686664356967e-07\n",
            "Epoch:  83 Batch:  20 Current Loss:  7.047123773418207e-08\n",
            "Epoch:  83 Batch:  25 Current Loss:  1.8247810658067465e-05\n",
            "Epoch:  83 Batch:  30 Current Loss:  1.1688419363053981e-05\n",
            "Epoch:  83 Batch:  35 Current Loss:  5.62099694434437e-06\n",
            "Epoch:  83 Batch:  40 Current Loss:  1.4751547496416606e-05\n",
            "Epoch:  83 Batch:  45 Current Loss:  4.36470463682781e-06\n",
            "Epoch:  83 Batch:  50 Current Loss:  8.024544513318688e-07\n",
            "================================================================================\n",
            "Epoch 83 completed\n",
            "Average train loss is 7.0814987375198934e-06: \n",
            "Average validation loss is 0.06209590705111623\n",
            "================================================================================\n",
            "Epoch:  84 Batch:  0 Current Loss:  2.3661755221837666e-07\n",
            "Epoch:  84 Batch:  5 Current Loss:  1.991755561903119e-05\n",
            "Epoch:  84 Batch:  10 Current Loss:  3.925638054624869e-07\n",
            "Epoch:  84 Batch:  15 Current Loss:  5.650747425534064e-06\n",
            "Epoch:  84 Batch:  20 Current Loss:  1.806140244298149e-05\n",
            "Epoch:  84 Batch:  25 Current Loss:  2.01447119252407e-06\n",
            "Epoch:  84 Batch:  30 Current Loss:  2.056393896054942e-06\n",
            "Epoch:  84 Batch:  35 Current Loss:  4.125983650737908e-06\n",
            "Epoch:  84 Batch:  40 Current Loss:  2.5956191166187637e-05\n",
            "Epoch:  84 Batch:  45 Current Loss:  2.9871109745727153e-06\n",
            "Epoch:  84 Batch:  50 Current Loss:  5.22950756476348e-07\n",
            "================================================================================\n",
            "Epoch 84 completed\n",
            "Average train loss is 7.539202091856921e-06: \n",
            "Average validation loss is 0.073910735702763\n",
            "================================================================================\n",
            "Epoch:  85 Batch:  0 Current Loss:  1.0169895176659338e-05\n",
            "Epoch:  85 Batch:  5 Current Loss:  5.036252332502045e-06\n",
            "Epoch:  85 Batch:  10 Current Loss:  1.0347825991630089e-06\n",
            "Epoch:  85 Batch:  15 Current Loss:  7.262657959472563e-07\n",
            "Epoch:  85 Batch:  20 Current Loss:  1.599444203748135e-06\n",
            "Epoch:  85 Batch:  25 Current Loss:  2.2579901326480467e-07\n",
            "Epoch:  85 Batch:  30 Current Loss:  1.6056620211202244e-07\n",
            "Epoch:  85 Batch:  35 Current Loss:  5.6020767260633875e-06\n",
            "Epoch:  85 Batch:  40 Current Loss:  1.032752493301814e-06\n",
            "Epoch:  85 Batch:  45 Current Loss:  1.4472443865543028e-07\n",
            "Epoch:  85 Batch:  50 Current Loss:  3.063516373913444e-08\n",
            "================================================================================\n",
            "Epoch 85 completed\n",
            "Average train loss is 5.3080742571974215e-06: \n",
            "Average validation loss is 0.07767351802128057\n",
            "================================================================================\n",
            "Epoch:  86 Batch:  0 Current Loss:  1.2325353964115493e-06\n",
            "Epoch:  86 Batch:  5 Current Loss:  1.0990509053954156e-06\n",
            "Epoch:  86 Batch:  10 Current Loss:  2.416893494228134e-07\n",
            "Epoch:  86 Batch:  15 Current Loss:  4.4525458875455115e-09\n",
            "Epoch:  86 Batch:  20 Current Loss:  6.13193762433184e-08\n",
            "Epoch:  86 Batch:  25 Current Loss:  5.46458011285722e-07\n",
            "Epoch:  86 Batch:  30 Current Loss:  8.944065399418832e-08\n",
            "Epoch:  86 Batch:  35 Current Loss:  3.569072987374966e-06\n",
            "Epoch:  86 Batch:  40 Current Loss:  1.9313608490278966e-09\n",
            "Epoch:  86 Batch:  45 Current Loss:  9.265606422559358e-06\n",
            "Epoch:  86 Batch:  50 Current Loss:  2.0824982129852287e-05\n",
            "================================================================================\n",
            "Epoch 86 completed\n",
            "Average train loss is 6.75977914238884e-06: \n",
            "Average validation loss is 0.06876554076249401\n",
            "================================================================================\n",
            "Epoch:  87 Batch:  0 Current Loss:  2.447155793561251e-06\n",
            "Epoch:  87 Batch:  5 Current Loss:  1.9457191228866577e-05\n",
            "Epoch:  87 Batch:  10 Current Loss:  6.486816639394988e-11\n",
            "Epoch:  87 Batch:  15 Current Loss:  3.2252135042654118e-06\n",
            "Epoch:  87 Batch:  20 Current Loss:  1.1555223863979336e-05\n",
            "Epoch:  87 Batch:  25 Current Loss:  3.335865983444819e-08\n",
            "Epoch:  87 Batch:  30 Current Loss:  5.411161964730127e-06\n",
            "Epoch:  87 Batch:  35 Current Loss:  6.293848855420947e-07\n",
            "Epoch:  87 Batch:  40 Current Loss:  4.686986812885152e-06\n",
            "Epoch:  87 Batch:  45 Current Loss:  2.4217206373577937e-06\n",
            "Epoch:  87 Batch:  50 Current Loss:  3.935192580684088e-06\n",
            "================================================================================\n",
            "Epoch 87 completed\n",
            "Average train loss is 6.172279421398214e-06: \n",
            "Average validation loss is 0.06952846562489867\n",
            "================================================================================\n",
            "Epoch:  88 Batch:  0 Current Loss:  2.4101878807414323e-05\n",
            "Epoch:  88 Batch:  5 Current Loss:  1.823909769882448e-06\n",
            "Epoch:  88 Batch:  10 Current Loss:  1.7696091163088568e-05\n",
            "Epoch:  88 Batch:  15 Current Loss:  7.160711135156816e-08\n",
            "Epoch:  88 Batch:  20 Current Loss:  5.132683781994274e-06\n",
            "Epoch:  88 Batch:  25 Current Loss:  3.361210474395193e-07\n",
            "Epoch:  88 Batch:  30 Current Loss:  1.1957305105170235e-05\n",
            "Epoch:  88 Batch:  35 Current Loss:  3.3230323879251955e-06\n",
            "Epoch:  88 Batch:  40 Current Loss:  8.09701674597818e-08\n",
            "Epoch:  88 Batch:  45 Current Loss:  8.004001728068033e-08\n",
            "Epoch:  88 Batch:  50 Current Loss:  2.669765706286853e-07\n",
            "================================================================================\n",
            "Epoch 88 completed\n",
            "Average train loss is 4.647411339725887e-06: \n",
            "Average validation loss is 0.07435175186643998\n",
            "================================================================================\n",
            "Epoch:  89 Batch:  0 Current Loss:  2.02416936190275e-06\n",
            "Epoch:  89 Batch:  5 Current Loss:  3.6431792977964506e-05\n",
            "Epoch:  89 Batch:  10 Current Loss:  9.043003046826925e-06\n",
            "Epoch:  89 Batch:  15 Current Loss:  1.87775162885373e-06\n",
            "Epoch:  89 Batch:  20 Current Loss:  9.673344720795285e-06\n",
            "Epoch:  89 Batch:  25 Current Loss:  7.88049931088608e-07\n",
            "Epoch:  89 Batch:  30 Current Loss:  1.476251441090426e-06\n",
            "Epoch:  89 Batch:  35 Current Loss:  1.6020081261558516e-08\n",
            "Epoch:  89 Batch:  40 Current Loss:  4.244259343977319e-06\n",
            "Epoch:  89 Batch:  45 Current Loss:  6.267285243666265e-06\n",
            "Epoch:  89 Batch:  50 Current Loss:  4.154302587267011e-06\n",
            "================================================================================\n",
            "Epoch 89 completed\n",
            "Average train loss is 5.916105405926549e-06: \n",
            "Average validation loss is 0.06765285972505808\n",
            "================================================================================\n",
            "Epoch:  90 Batch:  0 Current Loss:  7.86463374424784e-07\n",
            "Epoch:  90 Batch:  5 Current Loss:  5.315503130987054e-06\n",
            "Epoch:  90 Batch:  10 Current Loss:  1.2497190255089663e-05\n",
            "Epoch:  90 Batch:  15 Current Loss:  2.940143531304784e-05\n",
            "Epoch:  90 Batch:  20 Current Loss:  1.6836319218782592e-06\n",
            "Epoch:  90 Batch:  25 Current Loss:  9.449966455576941e-06\n",
            "Epoch:  90 Batch:  30 Current Loss:  8.82326094142627e-06\n",
            "Epoch:  90 Batch:  35 Current Loss:  3.147197844555194e-07\n",
            "Epoch:  90 Batch:  40 Current Loss:  5.673333816957893e-06\n",
            "Epoch:  90 Batch:  45 Current Loss:  1.6900185073609464e-06\n",
            "Epoch:  90 Batch:  50 Current Loss:  1.9738437913474627e-05\n",
            "================================================================================\n",
            "Epoch 90 completed\n",
            "Average train loss is 6.288705967049369e-06: \n",
            "Average validation loss is 0.06772015600775678\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Test Data (Grad-CAM, UNet)"
      ],
      "metadata": {
        "id": "R0po15FNy1Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grad-CAM testing\n",
        "normalization_factors = 0.5\n",
        "tx_X = transforms.Compose([transforms.Resize((256, 256)),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize(normalization_factors, normalization_factors)])\n",
        "REGION = \"Q\"\n",
        "OUTPUT = \"FM\"\n",
        "test_dataset = TestPatientDataset(transform = tx_X,\n",
        "                 augmented_dataset = False,\n",
        "                 augment = 0,\n",
        "                 threshold = False,\n",
        "                 speckle = True,\n",
        "                 despeckle = False,\n",
        "                 region_combination = REGION,\n",
        "                 number_of_image = 2,\n",
        "                 crop = [1, 1, 1],\n",
        "                 output = OUTPUT)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "print(len(test_loader))\n",
        "\n",
        "true_labels = []\n",
        "predictions = []\n",
        "WL = False\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "with torch.no_grad():\n",
        "    for ith_batch, batch in enumerate(test_loader):\n",
        "        # Convert the list of images to a single tensor and move to GPU\n",
        "        # Check if batch[0] is a list of tensors and stack them correctly\n",
        "        if isinstance(batch[0], list):\n",
        "            X_train = torch.stack(batch[0]).to('cuda')\n",
        "        else:\n",
        "            X_train = batch[0].to('cuda')\n",
        "\n",
        "        y_train = batch[1].to('cuda').float()  # Move the batch of labels to GPU and convert to float\n",
        "\n",
        "        # Ensure X_train has the correct shape: [batch_size, channels, height, width]\n",
        "        if X_train.dim() == 5:\n",
        "            # If the tensor has an extra dimension, remove it\n",
        "            X_train = X_train.view(-1, *X_train.shape[2:])\n",
        "\n",
        "        if WL:\n",
        "            if MODEL == \"EffNet_LP\" or MODEL == \"EffNet_FT\":\n",
        "                additional_data_train = torch.tensor([[batch[2].item(), batch[3].item()]]).to('cuda').float()\n",
        "                y_pred = model(X_train, additional_data_train)\n",
        "            elif MODEL == \"UNet\":\n",
        "                weight = batch[2].to('cuda').float()\n",
        "                length = batch[3].to('cuda').float()\n",
        "                y_pred = model(X_train, weight, length)\n",
        "        else:\n",
        "            y_pred = model(X_train)\n",
        "\n",
        "        # Append true labels and predictions\n",
        "        true_labels.append(y_train)\n",
        "        predictions.append(y_pred)\n",
        "\n",
        "# Convert lists to tensors\n",
        "true_labels = torch.cat(true_labels)\n",
        "predictions = torch.cat(predictions)\n",
        "\n",
        "# If necessary, move predictions and labels back to CPU\n",
        "true_labels = true_labels.cpu().numpy()\n",
        "predictions = predictions.cpu().numpy()\n",
        "\n",
        "mae = np.mean(np.abs(predictions - true_labels))\n",
        "mse = np.mean((predictions - true_labels) ** 2)\n",
        "rmse = np.sqrt(mse)\n",
        "mape = np.mean(np.abs((true_labels - predictions) / true_labels)) * 100\n",
        "\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"MAPE: {mape}\")\n",
        "\n",
        "print(\"\\nFirst five true labels and predictions:\")\n",
        "for i in range(min(5, len(true_labels))):\n",
        "    print(f\"True label: {true_labels[i]}, Prediction: {predictions[i]}\")"
      ],
      "metadata": {
        "id": "pITDqMp_y4NV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8a4910-4997-4359-dc4b-178e489f8081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "MAE: 0.11215952783823013\n",
            "MSE: 0.019884878769516945\n",
            "RMSE: 0.1410137563943863\n",
            "MAPE: 67.25102996826172\n",
            "\n",
            "First five true labels and predictions:\n",
            "True label: 0.18799999356269836, Prediction: [0.23783615]\n",
            "True label: 0.11249999701976776, Prediction: [0.10962443]\n",
            "True label: 0.15880000591278076, Prediction: [0.1736444]\n",
            "True label: 0.1216999962925911, Prediction: [0.19718358]\n",
            "True label: 0.22089999914169312, Prediction: [0.04468233]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grad-CAM\n",
        "The following section implements Grad-CAM for UNet.\n",
        "GradCAM registers forward/backward hooks on a chosen conv layer to capture activations (B,C,H,W) and gradients, then computes a heatmap by global-averaging the grads to get channel weights and forming a weighted sum over activations, ReLU, and [0–1] normalization. apply_grad_cam preprocesses a single image (256×256, normalized), generates the Grad-CAM, overlays it on the RGB image, and saves to {out_dir}/{layer}/{body_part}/. run_gradcam_for_body_part iterates all patient folders, matches images by body-part pattern (A/B/Q), applies Grad-CAM, and writes outputs—providing a reproducible pipeline to visualize which regions drive the scalar FM/FFM prediction."
      ],
      "metadata": {
        "id": "IfCL6BX4y8Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class GradCAM:\n",
        "    \"\"\"\n",
        "      Generate and apply Grad-CAM visualizations for a trained deep learning model.\n",
        "\n",
        "      The GradCAM class registers forward and backward hooks on a chosen layer\n",
        "      to capture feature map activations and gradients during backpropagation.\n",
        "      It then computes a heatmap highlighting important regions that contribute\n",
        "      to the model’s scalar regression output (FM or FFM).\n",
        "\n",
        "      The apply_grad_cam function preprocesses a single image, generates the CAM\n",
        "      from the specified layer, overlays the heatmap onto the original image, and\n",
        "      saves the result to disk.\n",
        "\n",
        "      Args:\n",
        "          model (nn.Module): Trained PyTorch model for which Grad-CAM is generated.\n",
        "          layer_name (str): Name of the convolutional layer to target for CAM.\n",
        "          image_path (str): Path to the input image.\n",
        "          body_part (str): Region label (e.g., \"A\" for Abdomen, \"B\" for Biceps, \"Q\" for Quadriceps).\n",
        "          out_dir (str): Directory where Grad-CAM overlay images are saved.\n",
        "\n",
        "      Returns:\n",
        "          np.ndarray (GradCAM.generate): Normalized heatmap (H, W) in [0,1].\n",
        "          str (apply_grad_cam): File path to the saved Grad-CAM overlay image.\n",
        "  \"\"\"\n",
        "    def __init__(self, model, layer_name: str):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        # Resolve the real module instance from the string\n",
        "        try:\n",
        "            self.target_layer = self.model.get_submodule(layer_name)\n",
        "        except AttributeError:\n",
        "            # For older PyTorch, fall back to manual walk\n",
        "            mod = self.model\n",
        "            for part in layer_name.split('.'):\n",
        "                mod = getattr(mod, part)\n",
        "            self.target_layer = mod\n",
        "\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        # Keep handles so they don't get GC'd\n",
        "        self._fwd_handle = self.target_layer.register_forward_hook(self._forward_hook)\n",
        "        self._bwd_handle = self.target_layer.register_full_backward_hook(self._backward_hook)\n",
        "\n",
        "    def _forward_hook(self, module, inputs, output):\n",
        "        # Save feature maps (B, C, H, W)\n",
        "        self.activations = output.detach()\n",
        "\n",
        "    def _backward_hook(self, module, grad_input, grad_output):\n",
        "        # grad_output is a tuple; the first element is dL/d(activations)\n",
        "        self.gradients = grad_output[0].detach()\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        self._fwd_handle.remove()\n",
        "        self._bwd_handle.remove()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _normalize_cam(self, cam):\n",
        "        cam -= cam.min()\n",
        "        denom = cam.max() + 1e-8\n",
        "        cam /= denom\n",
        "        return cam\n",
        "\n",
        "    def generate(self, input_tensor):\n",
        "        \"\"\"\n",
        "        input_tensor: (1, C, H, W) on same device as model\n",
        "        Returns: np.ndarray heatmap (H, W) in [0,1]\n",
        "        \"\"\"\n",
        "        # Forward\n",
        "        self.model.zero_grad(set_to_none=True)\n",
        "        output = self.model(input_tensor)        # regression: shape (1, 1) or (1,)\n",
        "        # Backward: for regression, push gradient of 1 through the scalar\n",
        "        if output.dim() == 2:        # (B,1)\n",
        "            grad_target = torch.ones_like(output)\n",
        "        else:                        # (B,)\n",
        "            grad_target = torch.ones_like(output).unsqueeze(-1)\n",
        "        output.backward(grad_target, retain_graph=False)\n",
        "\n",
        "        # Fetch grads & acts\n",
        "        grads = self.gradients      # (1, C, H, W)\n",
        "        acts  = self.activations    # (1, C, H, W)\n",
        "        if grads is None or acts is None:\n",
        "            raise RuntimeError(\"Hooks did not capture gradients/activations. \"\n",
        "                               \"Ensure the chosen layer is a conv layer and \"\n",
        "                               \"no .detach() is called before backward.\")\n",
        "\n",
        "        # Global average pooling over spatial dims -> weights (1, C, 1, 1)\n",
        "        weights = grads.mean(dim=(2, 3), keepdim=True)\n",
        "        # Weighted combination\n",
        "        cam = (weights * acts).sum(dim=1, keepdim=False)    # (1, H, W)\n",
        "        cam = F.relu(cam)[0].cpu().numpy()                  # (H, W)\n",
        "        cam = self._normalize_cam(cam)\n",
        "        return cam\n",
        "\n",
        "def apply_grad_cam(image_path, model, layer_name, body_part, out_dir):\n",
        "    device = next(model.parameters()).device\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
        "    ])\n",
        "    x = transform(image).unsqueeze(0).to(device)  # (1, C, H, W)\n",
        "\n",
        "    cam_engine = GradCAM(model, layer_name)\n",
        "    try:\n",
        "        cam = cam_engine.generate(x)  # (H, W) in [0,1]\n",
        "    finally:\n",
        "        cam_engine.remove_hooks()\n",
        "\n",
        "    # Overlay\n",
        "    img_np = np.array(image.resize((256,256)))\n",
        "    heatmap = (cam * 255.0).astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    overlay = cv2.addWeighted(img_np, 0.5, heatmap, 0.5, 0)\n",
        "\n",
        "    # Save\n",
        "    os.makedirs(os.path.join(out_dir, layer_name, body_part), exist_ok=True)\n",
        "    out_path = os.path.join(out_dir, layer_name, body_part,\n",
        "                            f\"Grad-CAM_{os.path.basename(image_path)}\")\n",
        "    cv2.imwrite(out_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
        "    return out_path"
      ],
      "metadata": {
        "id": "-OfJIbRUy9aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Map flexible user input → filename pattern + folder name\n",
        "_BODY_PART_MAP = {\n",
        "    \"A\":   (\"_A\",   \"A\"),\n",
        "    \"ABD\": (\"_A\",   \"A\"),\n",
        "    \"ABDOMEN\": (\"_A\",\"A\"),\n",
        "    \"B\":   (\"_B\",   \"B\"),\n",
        "    \"BICEP\": (\"_B\", \"B\"),\n",
        "    \"Q\":   (\"_Q\",   \"Q\"),\n",
        "    \"QUAD\":(\"_Q\",   \"Q\"),\n",
        "    \"QUADRICEPS\": (\"_Q\",\"Q\"),\n",
        "}\n",
        "\n",
        "def run_gradcam_for_body_part(model, layer_name, body_part, patients,\n",
        "                              root_dir=\"/content/gdrive/MyDrive/Ultrasound Files- Minnesota + Boston Collaboration/cropped_images\",\n",
        "                              out_dir=\"/content/gdrive/MyDrive/Ultrasound Files- Minnesota + Boston Collaboration/Grad-CAM/just_testing_code_2\"):\n",
        "    \"\"\"\n",
        "    Runs Grad-CAM only on the given list of test patients.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Trained model to explain.\n",
        "        layer_name (str): Layer name to target for CAM (e.g., \"outc\").\n",
        "        body_part (str): Region (\"A\", \"B\", \"Q\").\n",
        "        patients (list[str]): List of patient IDs to process (e.g., PATIENTS).\n",
        "        root_dir (str): Root directory of cropped_images.\n",
        "        out_dir (str): Directory where Grad-CAM results are saved.\n",
        "    \"\"\"\n",
        "    # Normalize body_part\n",
        "    key = str(body_part).strip().upper()\n",
        "    if key not in _BODY_PART_MAP:\n",
        "        raise ValueError(f\"Unknown body_part={body_part}. Use one of: {list(_BODY_PART_MAP.keys())}\")\n",
        "    pattern, part_folder = _BODY_PART_MAP[key]\n",
        "\n",
        "    save_dir = os.path.join(out_dir, layer_name, part_folder)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # uncomment below code if you want to run grad-cam on training data as well as testing data\n",
        "    #patients = [p for p in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, p))]\n",
        "    #patients.sort()\n",
        "\n",
        "    processed, errors = 0, 0\n",
        "    for pid in patients:\n",
        "        pdir = os.path.join(root_dir, pid)\n",
        "        # Find all images for this body part (in case there are multiple)\n",
        "        files = [f for f in os.listdir(pdir) if pattern in f]\n",
        "        if not files:\n",
        "            continue\n",
        "        for fname in files:\n",
        "            img_path = os.path.join(pdir, fname)\n",
        "            try:\n",
        "                out_path = apply_grad_cam(\n",
        "                    image_path=img_path,\n",
        "                    model=model,\n",
        "                    layer_name=layer_name,\n",
        "                    body_part=part_folder,\n",
        "                    out_dir=out_dir\n",
        "                )\n",
        "                print(f\"[OK] {pid} → {os.path.basename(out_path)}\")\n",
        "                processed += 1\n",
        "            except Exception as e:\n",
        "                print(f\"[ERR] {pid} / {fname}: {e}\")\n",
        "                errors += 1\n",
        "\n",
        "    print(f\"\\nDone. Processed: {processed}, Errors: {errors}, Saved to: {save_dir}\")\n",
        "\n",
        "# usage:\n",
        "run_gradcam_for_body_part(model, layer_name=\"outc\", body_part=\"Q\", patients=PATIENTS)"
      ],
      "metadata": {
        "id": "cufJlAbXy-3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af393e29-1bf7-4d7a-f948-164e18fa79bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] 17042418 → Grad-CAM_17042418_QUAD1_R.jpg\n",
            "[OK] 17042418 → Grad-CAM_17042418_QUAD3_R.jpg\n",
            "[OK] 17042418 → Grad-CAM_17042418_QUAD4_R.jpg\n",
            "[OK] 17042418 → Grad-CAM_17042418_QUAD2_R.jpg\n",
            "[OK] 17042418 → Grad-CAM_17042418_QUAD5_R.jpg\n",
            "[OK] 9021218 → Grad-CAM_9021218_QUAD1_R.jpg\n",
            "[OK] 9021218 → Grad-CAM_9021218_QUAD2_R.jpg\n",
            "[OK] 9021218 → Grad-CAM_9021218_QUAD4_R.jpg\n",
            "[OK] 9021218 → Grad-CAM_9021218_QUAD3_R.jpg\n",
            "[OK] 35080318 → Grad-CAM_35080318_QUAD4_R.jpg\n",
            "[OK] 35080318 → Grad-CAM_35080318_QUAD1_R.jpg\n",
            "[OK] 35080318 → Grad-CAM_35080318_QUAD3_R.jpg\n",
            "[OK] 35080318 → Grad-CAM_35080318_QUAD2_R.jpg\n",
            "[OK] 58041919 → Grad-CAM_58041919_QUAD3_R.jpg\n",
            "[OK] 58041919 → Grad-CAM_58041919_QUAD1_R.jpg\n",
            "[OK] 58041919 → Grad-CAM_58041919_QUAD2_R.jpg\n",
            "[OK] 65050619 → Grad-CAM_65050619_QUAD2_R.jpg\n",
            "[OK] 65050619 → Grad-CAM_65050619_QUAD3_R.jpg\n",
            "[OK] 65050619 → Grad-CAM_65050619_QUAD1_R.jpg\n",
            "[OK] 57032919 → Grad-CAM_57032919_QUAD2_R.jpg\n",
            "[OK] 57032919 → Grad-CAM_57032919_QUAD5_R.jpg\n",
            "[OK] 57032919 → Grad-CAM_57032919_QUAD4_R.jpg\n",
            "[OK] 57032919 → Grad-CAM_57032919_QUAD1_R.jpg\n",
            "[OK] 57032919 → Grad-CAM_57032919_QUAD3_R.jpg\n",
            "[OK] 57032919 → Grad-CAM_57032919_QUAD6_R.jpg\n",
            "[OK] 50102518 → Grad-CAM_50102518_QUAD3_R.jpg\n",
            "[OK] 50102518 → Grad-CAM_50102518_QUAD1_R.jpg\n",
            "[OK] 50102518 → Grad-CAM_50102518_QUAD2_R.jpg\n",
            "\n",
            "Done. Processed: 28, Errors: 0, Saved to: /content/gdrive/MyDrive/Ultrasound Files- Minnesota + Boston Collaboration/Grad-CAM/just_testing_code_2/outc/Q\n"
          ]
        }
      ]
    }
  ]
}